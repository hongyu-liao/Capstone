{
  "schema_name": "DoclingDocument",
  "version": "1.5.0",
  "name": "Document",
  "furniture": {
    "self_ref": "#/furniture",
    "children": [],
    "content_layer": "furniture",
    "name": "_root_",
    "label": "unspecified"
  },
  "body": {
    "self_ref": "#/body",
    "children": [
      {
        "$ref": "#/pictures/0"
      },
      {
        "$ref": "#/pictures/1"
      },
      {
        "$ref": "#/texts/0"
      },
      {
        "$ref": "#/texts/1"
      },
      {
        "$ref": "#/groups/0"
      },
      {
        "$ref": "#/texts/6"
      },
      {
        "$ref": "#/texts/7"
      },
      {
        "$ref": "#/texts/8"
      },
      {
        "$ref": "#/texts/9"
      },
      {
        "$ref": "#/texts/10"
      },
      {
        "$ref": "#/texts/11"
      },
      {
        "$ref": "#/texts/12"
      },
      {
        "$ref": "#/texts/13"
      },
      {
        "$ref": "#/texts/14"
      },
      {
        "$ref": "#/pictures/2"
      },
      {
        "$ref": "#/texts/15"
      },
      {
        "$ref": "#/texts/16"
      },
      {
        "$ref": "#/texts/17"
      },
      {
        "$ref": "#/texts/18"
      },
      {
        "$ref": "#/texts/19"
      },
      {
        "$ref": "#/texts/20"
      },
      {
        "$ref": "#/texts/21"
      },
      {
        "$ref": "#/texts/22"
      },
      {
        "$ref": "#/texts/23"
      },
      {
        "$ref": "#/groups/1"
      },
      {
        "$ref": "#/texts/28"
      },
      {
        "$ref": "#/texts/29"
      },
      {
        "$ref": "#/texts/30"
      },
      {
        "$ref": "#/texts/31"
      },
      {
        "$ref": "#/texts/32"
      },
      {
        "$ref": "#/texts/33"
      },
      {
        "$ref": "#/texts/34"
      },
      {
        "$ref": "#/texts/35"
      },
      {
        "$ref": "#/texts/36"
      },
      {
        "$ref": "#/texts/37"
      },
      {
        "$ref": "#/texts/38"
      },
      {
        "$ref": "#/texts/39"
      },
      {
        "$ref": "#/texts/40"
      },
      {
        "$ref": "#/texts/41"
      },
      {
        "$ref": "#/texts/42"
      },
      {
        "$ref": "#/texts/43"
      },
      {
        "$ref": "#/texts/44"
      },
      {
        "$ref": "#/texts/45"
      },
      {
        "$ref": "#/texts/46"
      },
      {
        "$ref": "#/texts/47"
      },
      {
        "$ref": "#/texts/48"
      },
      {
        "$ref": "#/texts/49"
      },
      {
        "$ref": "#/texts/50"
      },
      {
        "$ref": "#/texts/51"
      },
      {
        "$ref": "#/texts/52"
      },
      {
        "$ref": "#/texts/53"
      },
      {
        "$ref": "#/texts/54"
      },
      {
        "$ref": "#/texts/55"
      },
      {
        "$ref": "#/texts/56"
      },
      {
        "$ref": "#/texts/57"
      },
      {
        "$ref": "#/texts/58"
      },
      {
        "$ref": "#/texts/59"
      },
      {
        "$ref": "#/texts/60"
      },
      {
        "$ref": "#/texts/61"
      },
      {
        "$ref": "#/texts/62"
      },
      {
        "$ref": "#/texts/63"
      },
      {
        "$ref": "#/pictures/3"
      },
      {
        "$ref": "#/texts/64"
      },
      {
        "$ref": "#/texts/65"
      },
      {
        "$ref": "#/texts/66"
      },
      {
        "$ref": "#/texts/67"
      },
      {
        "$ref": "#/texts/68"
      },
      {
        "$ref": "#/pictures/4"
      },
      {
        "$ref": "#/texts/69"
      },
      {
        "$ref": "#/texts/70"
      },
      {
        "$ref": "#/texts/71"
      },
      {
        "$ref": "#/texts/72"
      },
      {
        "$ref": "#/texts/73"
      },
      {
        "$ref": "#/tables/0"
      },
      {
        "$ref": "#/texts/74"
      },
      {
        "$ref": "#/tables/1"
      },
      {
        "$ref": "#/texts/75"
      },
      {
        "$ref": "#/texts/76"
      },
      {
        "$ref": "#/texts/77"
      },
      {
        "$ref": "#/texts/78"
      },
      {
        "$ref": "#/groups/2"
      },
      {
        "$ref": "#/texts/85"
      },
      {
        "$ref": "#/texts/86"
      },
      {
        "$ref": "#/texts/87"
      },
      {
        "$ref": "#/texts/88"
      },
      {
        "$ref": "#/texts/89"
      },
      {
        "$ref": "#/texts/90"
      },
      {
        "$ref": "#/texts/91"
      },
      {
        "$ref": "#/texts/92"
      },
      {
        "$ref": "#/texts/93"
      },
      {
        "$ref": "#/texts/94"
      },
      {
        "$ref": "#/texts/95"
      },
      {
        "$ref": "#/texts/96"
      },
      {
        "$ref": "#/texts/97"
      },
      {
        "$ref": "#/texts/98"
      },
      {
        "$ref": "#/texts/99"
      },
      {
        "$ref": "#/texts/100"
      },
      {
        "$ref": "#/texts/101"
      },
      {
        "$ref": "#/tables/2"
      },
      {
        "$ref": "#/texts/102"
      },
      {
        "$ref": "#/texts/103"
      },
      {
        "$ref": "#/texts/104"
      },
      {
        "$ref": "#/tables/3"
      },
      {
        "$ref": "#/texts/105"
      },
      {
        "$ref": "#/texts/106"
      },
      {
        "$ref": "#/texts/107"
      },
      {
        "$ref": "#/texts/108"
      },
      {
        "$ref": "#/texts/109"
      },
      {
        "$ref": "#/texts/110"
      },
      {
        "$ref": "#/texts/111"
      },
      {
        "$ref": "#/texts/112"
      },
      {
        "$ref": "#/texts/113"
      },
      {
        "$ref": "#/texts/114"
      },
      {
        "$ref": "#/texts/115"
      },
      {
        "$ref": "#/texts/116"
      },
      {
        "$ref": "#/texts/117"
      },
      {
        "$ref": "#/groups/3"
      },
      {
        "$ref": "#/texts/119"
      },
      {
        "$ref": "#/texts/120"
      },
      {
        "$ref": "#/groups/4"
      },
      {
        "$ref": "#/texts/124"
      },
      {
        "$ref": "#/texts/125"
      },
      {
        "$ref": "#/texts/126"
      },
      {
        "$ref": "#/texts/127"
      },
      {
        "$ref": "#/texts/128"
      },
      {
        "$ref": "#/texts/129"
      },
      {
        "$ref": "#/texts/130"
      },
      {
        "$ref": "#/texts/131"
      },
      {
        "$ref": "#/texts/132"
      },
      {
        "$ref": "#/texts/133"
      },
      {
        "$ref": "#/texts/134"
      },
      {
        "$ref": "#/texts/135"
      },
      {
        "$ref": "#/texts/136"
      },
      {
        "$ref": "#/texts/137"
      },
      {
        "$ref": "#/texts/138"
      },
      {
        "$ref": "#/texts/139"
      },
      {
        "$ref": "#/texts/140"
      },
      {
        "$ref": "#/pictures/5"
      },
      {
        "$ref": "#/pictures/6"
      },
      {
        "$ref": "#/texts/141"
      },
      {
        "$ref": "#/texts/142"
      },
      {
        "$ref": "#/pictures/7"
      },
      {
        "$ref": "#/texts/143"
      },
      {
        "$ref": "#/groups/5"
      },
      {
        "$ref": "#/texts/161"
      },
      {
        "$ref": "#/texts/162"
      },
      {
        "$ref": "#/groups/6"
      },
      {
        "$ref": "#/texts/190"
      },
      {
        "$ref": "#/texts/191"
      },
      {
        "$ref": "#/groups/7"
      },
      {
        "$ref": "#/texts/212"
      }
    ],
    "content_layer": "body",
    "name": "_root_",
    "label": "unspecified"
  },
  "groups": [
    {
      "self_ref": "#/groups/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/2"
        },
        {
          "$ref": "#/texts/3"
        },
        {
          "$ref": "#/texts/4"
        },
        {
          "$ref": "#/texts/5"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/24"
        },
        {
          "$ref": "#/texts/25"
        },
        {
          "$ref": "#/texts/26"
        },
        {
          "$ref": "#/texts/27"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/79"
        },
        {
          "$ref": "#/texts/80"
        },
        {
          "$ref": "#/texts/81"
        },
        {
          "$ref": "#/texts/82"
        },
        {
          "$ref": "#/texts/83"
        },
        {
          "$ref": "#/texts/84"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/118"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/121"
        },
        {
          "$ref": "#/texts/122"
        },
        {
          "$ref": "#/texts/123"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/144"
        },
        {
          "$ref": "#/texts/145"
        },
        {
          "$ref": "#/texts/146"
        },
        {
          "$ref": "#/texts/147"
        },
        {
          "$ref": "#/texts/148"
        },
        {
          "$ref": "#/texts/149"
        },
        {
          "$ref": "#/texts/150"
        },
        {
          "$ref": "#/texts/151"
        },
        {
          "$ref": "#/texts/152"
        },
        {
          "$ref": "#/texts/153"
        },
        {
          "$ref": "#/texts/154"
        },
        {
          "$ref": "#/texts/155"
        },
        {
          "$ref": "#/texts/156"
        },
        {
          "$ref": "#/texts/157"
        },
        {
          "$ref": "#/texts/158"
        },
        {
          "$ref": "#/texts/159"
        },
        {
          "$ref": "#/texts/160"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/163"
        },
        {
          "$ref": "#/texts/164"
        },
        {
          "$ref": "#/texts/165"
        },
        {
          "$ref": "#/texts/166"
        },
        {
          "$ref": "#/texts/167"
        },
        {
          "$ref": "#/texts/168"
        },
        {
          "$ref": "#/texts/169"
        },
        {
          "$ref": "#/texts/170"
        },
        {
          "$ref": "#/texts/171"
        },
        {
          "$ref": "#/texts/172"
        },
        {
          "$ref": "#/texts/173"
        },
        {
          "$ref": "#/texts/174"
        },
        {
          "$ref": "#/texts/175"
        },
        {
          "$ref": "#/texts/176"
        },
        {
          "$ref": "#/texts/177"
        },
        {
          "$ref": "#/texts/178"
        },
        {
          "$ref": "#/texts/179"
        },
        {
          "$ref": "#/texts/180"
        },
        {
          "$ref": "#/texts/181"
        },
        {
          "$ref": "#/texts/182"
        },
        {
          "$ref": "#/texts/183"
        },
        {
          "$ref": "#/texts/184"
        },
        {
          "$ref": "#/texts/185"
        },
        {
          "$ref": "#/texts/186"
        },
        {
          "$ref": "#/texts/187"
        },
        {
          "$ref": "#/texts/188"
        },
        {
          "$ref": "#/texts/189"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    },
    {
      "self_ref": "#/groups/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [
        {
          "$ref": "#/texts/192"
        },
        {
          "$ref": "#/texts/193"
        },
        {
          "$ref": "#/texts/194"
        },
        {
          "$ref": "#/texts/195"
        },
        {
          "$ref": "#/texts/196"
        },
        {
          "$ref": "#/texts/197"
        },
        {
          "$ref": "#/texts/198"
        },
        {
          "$ref": "#/texts/199"
        },
        {
          "$ref": "#/texts/200"
        },
        {
          "$ref": "#/texts/201"
        },
        {
          "$ref": "#/texts/202"
        },
        {
          "$ref": "#/texts/203"
        },
        {
          "$ref": "#/texts/204"
        },
        {
          "$ref": "#/texts/205"
        },
        {
          "$ref": "#/texts/206"
        },
        {
          "$ref": "#/texts/207"
        },
        {
          "$ref": "#/texts/208"
        },
        {
          "$ref": "#/texts/209"
        },
        {
          "$ref": "#/texts/210"
        },
        {
          "$ref": "#/texts/211"
        }
      ],
      "content_layer": "body",
      "name": "list",
      "label": "list"
    }
  ],
  "texts": [
    {
      "self_ref": "#/texts/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 117.88000000000001,
            "r": 547.4,
            "b": 158.296,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            81
          ]
        }
      ],
      "orig": "Domain Adaptation for Arabic Machine Translation: Financial Texts as a Case Study",
      "text": "Domain Adaptation for Arabic Machine Translation: Financial Texts as a Case Study",
      "level": 1
    },
    {
      "self_ref": "#/texts/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 170.084,
            "r": 365.33,
            "b": 181.87199999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            78
          ]
        }
      ],
      "orig": "Emad A. Alghamdi 1,2, ∗ , t ∗ , Jezia Zakraoui 2, t ∗ and Fares A. Abanny 2, d",
      "text": "Emad A. Alghamdi 1,2, ∗ , t ∗ , Jezia Zakraoui 2, t ∗ and Fares A. Abanny 2, d"
    },
    {
      "self_ref": "#/texts/2",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 202.07999999999998,
            "r": 537.88,
            "b": 212.184,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            101
          ]
        }
      ],
      "orig": "1 Center of Excellence in AI and Data Sciences, King Abdulaziz University, Jeddah 21589, Saudi Arabia",
      "text": "1 Center of Excellence in AI and Data Sciences, King Abdulaziz University, Jeddah 21589, Saudi Arabia",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/3",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 213.868,
            "r": 330.82000000000005,
            "b": 223.972,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            41
          ]
        }
      ],
      "orig": "2 ASAS AI Lab, Riyadh 13518, Saudi Arabia",
      "text": "2 ASAS AI Lab, Riyadh 13518, Saudi Arabia",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/4",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 225.656,
            "r": 330.82000000000005,
            "b": 235.76000000000002,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            39
          ]
        }
      ],
      "orig": "∗ Correspondence: eaalghamdi@kau.edu.sa",
      "text": "∗ Correspondence: eaalghamdi@kau.edu.sa",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/5",
      "parent": {
        "$ref": "#/groups/0"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 235.76000000000002,
            "r": 348.66999999999996,
            "b": 245.86399999999998,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            49
          ]
        }
      ],
      "orig": "∗ These authors contributed equally to this work.",
      "text": "∗ These authors contributed equally to this work.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 261.02,
            "r": 559.3,
            "b": 452.99600000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1438
          ]
        }
      ],
      "orig": "Abstract: Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-ofdomain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaptation in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed a parallel corpus for Arabic-English (AR-EN) translation in the financial domain to benchmark different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that fine-tuning pre-trained NMT models on a few well-aligned in-domain AR-EN segments led to noticeable improvement. The quality of ChatGPT translation was superior to other models based on automatic and human evaluations. To the best of our knowledge, this is the first work on fine-tuning ChatGPT towards financial domain transfer learning. To contribute to research in domain translation, we made our datasets and fine-tuned models available.",
      "text": "Abstract: Neural machine translation (NMT) has shown impressive performance when trained on large-scale corpora. However, generic NMT systems have demonstrated poor performance on out-ofdomain translation. To mitigate this issue, several domain adaptation methods have recently been proposed which often lead to better translation quality than genetic NMT systems. While there has been some continuous progress in NMT for English and other European languages, domain adaptation in Arabic has received little attention in the literature. The current study, therefore, aims to explore the effectiveness of domain-specific adaptation for Arabic MT (AMT), in yet unexplored domain, financial news articles. To this end, we developed a parallel corpus for Arabic-English (AR-EN) translation in the financial domain to benchmark different domain adaptation methods. We then fine-tuned several pre-trained NMT and Large Language models including ChatGPT-3.5 Turbo on our dataset. The results showed that fine-tuning pre-trained NMT models on a few well-aligned in-domain AR-EN segments led to noticeable improvement. The quality of ChatGPT translation was superior to other models based on automatic and human evaluations. To the best of our knowledge, this is the first work on fine-tuning ChatGPT towards financial domain transfer learning. To contribute to research in domain translation, we made our datasets and fine-tuned models available."
    },
    {
      "self_ref": "#/texts/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 165.41000000000003,
            "t": 464.78400000000005,
            "r": 490.28,
            "b": 474.888,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            77
          ]
        }
      ],
      "orig": "Keywords: machine translation; Arabic MT; domain adaptation; financial domain",
      "text": "Keywords: machine translation; Arabic MT; domain adaptation; financial domain"
    },
    {
      "self_ref": "#/texts/8",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 483.30799999999994,
            "r": 84.49,
            "b": 498.464,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            17
          ]
        }
      ],
      "orig": "check for updates",
      "text": "check for updates"
    },
    {
      "self_ref": "#/texts/9",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 503.51599999999996,
            "r": 160.65,
            "b": 572.5600000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            201
          ]
        }
      ],
      "orig": "Citation: Alghamdi, E.A.; Zakraoui, J.; Abanny, F.A. Domain Adaptation for Arabic Machine Translation: Financial Texts as a Case Study. Appl. Sci. 2024 , 14 , 7088. https://doi.org/10.3390/ app14167088",
      "text": "Citation: Alghamdi, E.A.; Zakraoui, J.; Abanny, F.A. Domain Adaptation for Arabic Machine Translation: Financial Texts as a Case Study. Appl. Sci. 2024 , 14 , 7088. https://doi.org/10.3390/ app14167088"
    },
    {
      "self_ref": "#/texts/10",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 579.2959999999999,
            "r": 142.79999999999998,
            "b": 587.716,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            30
          ]
        }
      ],
      "orig": "Academic Editor: Tobias Meisen",
      "text": "Academic Editor: Tobias Meisen"
    },
    {
      "self_ref": "#/texts/11",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 599.504,
            "r": 108.28999999999999,
            "b": 607.924,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            22
          ]
        }
      ],
      "orig": "Received: 30 June 2024",
      "text": "Received: 30 June 2024"
    },
    {
      "self_ref": "#/texts/12",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 611.292,
            "r": 98.77000000000001,
            "b": 619.712,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "Revised: 24 July 2024",
      "text": "Revised: 24 July 2024"
    },
    {
      "self_ref": "#/texts/13",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 623.08,
            "r": 119.0,
            "b": 631.5,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            23
          ]
        }
      ],
      "orig": "Accepted: 1 August 2024",
      "text": "Accepted: 1 August 2024"
    },
    {
      "self_ref": "#/texts/14",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 636.552,
            "r": 119.0,
            "b": 644.972,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            25
          ]
        }
      ],
      "orig": "Published: 13 August 2024",
      "text": "Published: 13 August 2024"
    },
    {
      "self_ref": "#/texts/15",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 35.699999999999996,
            "t": 682.0200000000001,
            "r": 153.51,
            "b": 774.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            252
          ]
        }
      ],
      "orig": "Copyright: © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/).",
      "text": "Copyright: © 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (https:// creativecommons.org/licenses/by/ 4.0/)."
    },
    {
      "self_ref": "#/texts/16",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 177.31,
            "t": 532.144,
            "r": 559.3,
            "b": 771.272,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1656
          ]
        }
      ],
      "orig": "In recent years, the rapid advancement of deep learning techniques and their adaptation in machine translation has made a great stride in many translation tasks. Neural Machine Translation (NMT) systems trained on a large-scale corpus, have demonstrated impressive performance in translating generic language. However, NMT models tend to perform poorly on out-of-domain data [1], especially if the target domain has a distinctive style and vocabulary [2]. Accordingly, an NMT model trained on exclusively medical texts is unlikely to achieve accurate performance on financial or news data. To address this problem, researchers have proposed different domain adaptation adaptations approaches and techniques that seem to improve the quality of NMT systems on out-of-domain data [3-5]. There are many MT models, systems and tools for translating Arabic texts in the literature; however, the quality of the translation is poor, especially for out-of-domain texts [6,7]. A key technical challenge related to AMT arises from the lack of available bilingual datasets for out-of-domain texts that can be used as standard benchmarks to conduct unified experiments. In fact, researchers tend to collect datasets according to their specific domains and try to resolve the linguistic issues for Arabic, based on custom datasets such as in the domain of news [8,9], ignoring thereby many other domains. Other technical issues such as out-of-vocabulary (OOV) and very long sentences also make MT more challenging [1]. To address these challenges, researchers have proposed different techniques, including, for example, BPE [10], character-level BPE variant [11], hybrid",
      "text": "In recent years, the rapid advancement of deep learning techniques and their adaptation in machine translation has made a great stride in many translation tasks. Neural Machine Translation (NMT) systems trained on a large-scale corpus, have demonstrated impressive performance in translating generic language. However, NMT models tend to perform poorly on out-of-domain data [1], especially if the target domain has a distinctive style and vocabulary [2]. Accordingly, an NMT model trained on exclusively medical texts is unlikely to achieve accurate performance on financial or news data. To address this problem, researchers have proposed different domain adaptation adaptations approaches and techniques that seem to improve the quality of NMT systems on out-of-domain data [3-5]. There are many MT models, systems and tools for translating Arabic texts in the literature; however, the quality of the translation is poor, especially for out-of-domain texts [6,7]. A key technical challenge related to AMT arises from the lack of available bilingual datasets for out-of-domain texts that can be used as standard benchmarks to conduct unified experiments. In fact, researchers tend to collect datasets according to their specific domains and try to resolve the linguistic issues for Arabic, based on custom datasets such as in the domain of news [8,9], ignoring thereby many other domains. Other technical issues such as out-of-vocabulary (OOV) and very long sentences also make MT more challenging [1]. To address these challenges, researchers have proposed different techniques, including, for example, BPE [10], character-level BPE variant [11], hybrid"
    },
    {
      "self_ref": "#/texts/17",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_footer",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 34.510000000000005,
            "t": 815.0559999999999,
            "r": 258.23,
            "b": 825.16,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            64
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088. https://doi.org/10.3390/app14167088",
      "text": "Appl. Sci. 2024 , 14 , 7088. https://doi.org/10.3390/app14167088"
    },
    {
      "self_ref": "#/texts/18",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_footer",
      "prov": [
        {
          "page_no": 1,
          "bbox": {
            "l": 411.73999999999995,
            "t": 815.0559999999999,
            "r": 559.3,
            "b": 825.16,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            36
          ]
        }
      ],
      "orig": "https://www.mdpi.com/journal/applsci",
      "text": "https://www.mdpi.com/journal/applsci"
    },
    {
      "self_ref": "#/texts/19",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/20",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "2 of 15",
      "text": "2 of 15"
    },
    {
      "self_ref": "#/texts/21",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 559.3,
            "b": 153.244,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            388
          ]
        }
      ],
      "orig": "techniques [12], and mixed fine-tuning [6]. However, domain robustness remains an unsolved problem and there is a need for further research in this area [13]. This is especially true for the Arabic language. Existing domain adaptation research has only focused on news [14] and medical [7] domains, no prior study, to the best of our knowledge, has been conducted on the financial domain.",
      "text": "techniques [12], and mixed fine-tuning [6]. However, domain robustness remains an unsolved problem and there is a need for further research in this area [13]. This is especially true for the Arabic language. Existing domain adaptation research has only focused on news [14] and medical [7] domains, no prior study, to the best of our knowledge, has been conducted on the financial domain."
    },
    {
      "self_ref": "#/texts/22",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 165.41000000000003,
            "t": 154.928,
            "r": 560.49,
            "b": 264.388,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            745
          ]
        }
      ],
      "orig": "To alleviate the issue of translation mismatch related to out-of-domain texts, the authors in [14] studied the performance of NMT systems under morphology-based and frequency-based tokenization schemes and BPE on in-domain data. They evaluated their best-performing models on out-of-domain data yielding significant improvements of 37.96% in BLEU score [15]. Ref. [7] proposed a method for domain-specific data augmentation for MT to tackle the issue with a small bilingual dataset. They employed mixed fine-tuning to train models that significantly improve the translation of in-domain texts. Their method achieved improvements of approximately 5-6 BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic language pairs.",
      "text": "To alleviate the issue of translation mismatch related to out-of-domain texts, the authors in [14] studied the performance of NMT systems under morphology-based and frequency-based tokenization schemes and BPE on in-domain data. They evaluated their best-performing models on out-of-domain data yielding significant improvements of 37.96% in BLEU score [15]. Ref. [7] proposed a method for domain-specific data augmentation for MT to tackle the issue with a small bilingual dataset. They employed mixed fine-tuning to train models that significantly improve the translation of in-domain texts. Their method achieved improvements of approximately 5-6 BLEU and 2-3 BLEU, respectively, on the Arabic-to-English and English-to-Arabic language pairs."
    },
    {
      "self_ref": "#/texts/23",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 165.41000000000003,
            "t": 266.072,
            "r": 560.49,
            "b": 338.48400000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            458
          ]
        }
      ],
      "orig": "While a lot of research in domain adaptation in MT for other language pairs like [6], ref. [16] exists which focuses on synthetic data generation and multiple other techniques like checkpoint averaging [6], only one work [7] investigated the same for AMT, but only for a medical domain. Therefore, this research aims to fill the gap in evaluating different MT settings and investigate domain adaption for financial texts. Our contributions are the following:",
      "text": "While a lot of research in domain adaptation in MT for other language pairs like [6], ref. [16] exists which focuses on synthetic data generation and multiple other techniques like checkpoint averaging [6], only one work [7] investigated the same for AMT, but only for a medical domain. Therefore, this research aims to fill the gap in evaluating different MT settings and investigate domain adaption for financial texts. Our contributions are the following:"
    },
    {
      "self_ref": "#/texts/24",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 341.85200000000003,
            "r": 487.9,
            "b": 353.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            71
          ]
        }
      ],
      "orig": "· We introduce the first AR-EN parallel corpus in the financial domain.",
      "text": "· We introduce the first AR-EN parallel corpus in the financial domain.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/25",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 355.324,
            "r": 559.3,
            "b": 378.90000000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            118
          ]
        }
      ],
      "orig": "· We compare the effectiveness of different adaption methods and data augmentation approaches for limited domain data.",
      "text": "· We compare the effectiveness of different adaption methods and data augmentation approaches for limited domain data.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/26",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 380.584,
            "r": 559.3,
            "b": 404.15999999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            150
          ]
        }
      ],
      "orig": "· We fine-tuned several models and made them publicly available to the research community at https://huggingface.co/asai-/ (accessed on 20 July 2024).",
      "text": "· We fine-tuned several models and made them publicly available to the research community at https://huggingface.co/asai-/ (accessed on 20 July 2024).",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/27",
      "parent": {
        "$ref": "#/groups/1"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 405.844,
            "r": 559.3,
            "b": 429.42,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            102
          ]
        }
      ],
      "orig": "· Our work is the first to fine-tune the GPT3.5 model and evaluate its capability for domain adaption.",
      "text": "· Our work is the first to fine-tune the GPT3.5 model and evaluate its capability for domain adaption.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/28",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 439.524,
            "r": 233.24,
            "b": 449.62800000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "2. Background",
      "text": "2. Background",
      "level": 1
    },
    {
      "self_ref": "#/texts/29",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 166.60000000000002,
            "t": 454.68,
            "r": 298.69,
            "b": 464.78400000000005,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            31
          ]
        }
      ],
      "orig": "2.1. Neural Machine Translation",
      "text": "2.1. Neural Machine Translation",
      "level": 1
    },
    {
      "self_ref": "#/texts/30",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 165.41000000000003,
            "t": 469.83600000000007,
            "r": 560.49,
            "b": 607.924,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1029
          ]
        }
      ],
      "orig": "NMT models based on deep neural networks (DNN) have been proposed in early NMT research [17]. A DNN-based NMT model employs a neural network system to perform the required machine translation tasks using an encoder-decoder network [18]. The encoder neural network inputs and encodes a source language sentence into a fixed-length vector in each hidden state. Then, given the final hidden state of the encoder, the decoder does the reverse work by transforming the hidden state vector to the target sentence word by word. A translation probability of a source sentence is modeled into the target sentence. Given a source sentence S = { s$_{1}$, s$_{2}$, . . . } and a target sentence T = { t$_{1}$, t$_{2}$, . . .n } , the encoder encodes all the words from the source sentence S into a set of hidden states ( h$_{1}$, h$_{2}$, . . .h$_{n}$ ) and passes the fixed-size vector v , which represents the source sentence, to the decoder. The translation probability with a single neural network is given by the following formula [19]:",
      "text": "NMT models based on deep neural networks (DNN) have been proposed in early NMT research [17]. A DNN-based NMT model employs a neural network system to perform the required machine translation tasks using an encoder-decoder network [18]. The encoder neural network inputs and encodes a source language sentence into a fixed-length vector in each hidden state. Then, given the final hidden state of the encoder, the decoder does the reverse work by transforming the hidden state vector to the target sentence word by word. A translation probability of a source sentence is modeled into the target sentence. Given a source sentence S = { s$_{1}$, s$_{2}$, . . . } and a target sentence T = { t$_{1}$, t$_{2}$, . . .n } , the encoder encodes all the words from the source sentence S into a set of hidden states ( h$_{1}$, h$_{2}$, . . .h$_{n}$ ) and passes the fixed-size vector v , which represents the source sentence, to the decoder. The translation probability with a single neural network is given by the following formula [19]:"
    },
    {
      "self_ref": "#/texts/31",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "formula",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 318.92,
            "t": 618.028,
            "r": 559.3,
            "b": 643.288,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            75
          ]
        }
      ],
      "orig": "P ( S ) = \\prod _ { i = 1 } ^ { n } P ( t _ { i } \\leqslant \\mathbb { R } )",
      "text": "P ( S ) = \\prod _ { i = 1 } ^ { n } P ( t _ { i } \\leqslant \\mathbb { R } )"
    },
    {
      "self_ref": "#/texts/32",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 2,
          "bbox": {
            "l": 165.41000000000003,
            "t": 650.024,
            "r": 560.49,
            "b": 774.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "where t",
      "text": "where t"
    },
    {
      "self_ref": "#/texts/33",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/34",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 534.3100000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "3 of 15",
      "text": "3 of 15"
    },
    {
      "self_ref": "#/texts/35",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 559.3,
            "b": 126.3,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            220
          ]
        }
      ],
      "orig": "the input sequences using score functions [20]. There are three different architectures for constructing NMT, namely Recurrent neural network (RNN), Convolution neural network (CNN), and Self-attention-based Transformer.",
      "text": "the input sequences using score functions [20]. There are three different architectures for constructing NMT, namely Recurrent neural network (RNN), Convolution neural network (CNN), and Self-attention-based Transformer."
    },
    {
      "self_ref": "#/texts/36",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 127.984,
            "r": 560.49,
            "b": 178.504,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            279
          ]
        }
      ],
      "orig": "The use of RNN-based models has demonstrated good-quality translation results. This type of network is composed of an encoder and decoder with similar uses of sequenceto-sequence learning. Multiple variants of RNN architectures include, i.e., LSTM [21], BiLSTM [20] and GRU [22].",
      "text": "The use of RNN-based models has demonstrated good-quality translation results. This type of network is composed of an encoder and decoder with similar uses of sequenceto-sequence learning. Multiple variants of RNN architectures include, i.e., LSTM [21], BiLSTM [20] and GRU [22]."
    },
    {
      "self_ref": "#/texts/37",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 178.504,
            "r": 560.49,
            "b": 262.704,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            533
          ]
        }
      ],
      "orig": "The second approach to developing NMT systems is based on convolution neural network (CNN) architecture. Work using CNN has generally reported good results, especially for word-based MT [23]. This work applied a convolution layer on the bottom of the recurrent layer which hinders the performance. The bottleneck was handled by implementing the fully convolutional model as suggested by [24]. The performance and accuracy were improved with a number of models; word-based [25], character-based [11], and recovery with attention [26].",
      "text": "The second approach to developing NMT systems is based on convolution neural network (CNN) architecture. Work using CNN has generally reported good results, especially for word-based MT [23]. This work applied a convolution layer on the bottom of the recurrent layer which hinders the performance. The bottleneck was handled by implementing the fully convolutional model as suggested by [24]. The performance and accuracy were improved with a number of models; word-based [25], character-based [11], and recovery with attention [26]."
    },
    {
      "self_ref": "#/texts/38",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 264.388,
            "r": 560.49,
            "b": 375.532,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            749
          ]
        }
      ],
      "orig": "Recently, the use of transformers has resulted in well-performing machine translation systems. This model is a sequence-to-sequence model [27], which consists of a stack of layers. Each layer first utilizes self-attention to extract information from the whole sentence and then follows a point-wise feed-forward network to provide non-linearity. The novel idea of self-attention is to extend the mechanism to the processing of input sequences and output sentences as well. In general form, the Transformer attention function uses three vectors: queries (Q), keys (K) and values (V). The output is a weighted sum of values, where weights are computed by a similarity score between n query vectors and m keys [27]. The attention is defined as follows:",
      "text": "Recently, the use of transformers has resulted in well-performing machine translation systems. This model is a sequence-to-sequence model [27], which consists of a stack of layers. Each layer first utilizes self-attention to extract information from the whole sentence and then follows a point-wise feed-forward network to provide non-linearity. The novel idea of self-attention is to extend the mechanism to the processing of input sequences and output sentences as well. In general form, the Transformer attention function uses three vectors: queries (Q), keys (K) and values (V). The output is a weighted sum of values, where weights are computed by a similarity score between n query vectors and m keys [27]. The attention is defined as follows:"
    },
    {
      "self_ref": "#/texts/39",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "formula",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 260.61,
            "t": 387.32,
            "r": 559.3,
            "b": 399.108,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            119
          ]
        }
      ],
      "orig": "A t t e n t { \\mathrm { o n t i e n } } \\left ( Q, K, V \\right ) = s o f m a t { \\mathrm { x } } ( s o r e ( Q, K ) ) V",
      "text": "A t t e n t { \\mathrm { o n t i e n } } \\left ( Q, K, V \\right ) = s o f m a t { \\mathrm { x } } ( s o r e ( Q, K ) ) V"
    },
    {
      "self_ref": "#/texts/40",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 410.896,
            "r": 559.3,
            "b": 484.99199999999996,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            487
          ]
        }
      ],
      "orig": "where score Q , K is an n × m matrix of similarity scores. A straightforward choice for score Q , K proposed by Luong et al. [28] is the dot product, i.e., score ( Q , K ) = QK . The softmax function normalizes over the columns of that matrix so that the weights for each query vector sum up to one. There are many variants in the implementation of attention-based models which are classified into two broad categories, global and local attention discussed in detail in this survey [17].",
      "text": "where score Q , K is an n × m matrix of similarity scores. A straightforward choice for score Q , K proposed by Luong et al. [28] is the dot product, i.e., score ( Q , K ) = QK . The softmax function normalizes over the columns of that matrix so that the weights for each query vector sum up to one. There are many variants in the implementation of attention-based models which are classified into two broad categories, global and local attention discussed in detail in this survey [17]."
    },
    {
      "self_ref": "#/texts/41",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 486.676,
            "r": 560.49,
            "b": 611.292,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            835
          ]
        }
      ],
      "orig": "The current state-of-the-art NMT models [29] rely on the Transformer model [27] and multiple attention mechanism [20]. However, transformer-based language models such as Bidirectional Encoder Representation from Transformers (BERT) [30] expand the function of attention to encompass the main task. It uses self-attention, which is applied to two states within the same sequence, as the foundation for sequence representations rather than an RNN. For the Arabic language, two transformer-based language models have been developed so far; notably AraBERT [31] and GigBAERT [32]. Both models aim at solving a masked language-modeling task in order to correctly predict a masked word from its context. Besides, these models aim at resolving a next sentence prediction task especially to decide whether two sentences are consecutive or not.",
      "text": "The current state-of-the-art NMT models [29] rely on the Transformer model [27] and multiple attention mechanism [20]. However, transformer-based language models such as Bidirectional Encoder Representation from Transformers (BERT) [30] expand the function of attention to encompass the main task. It uses self-attention, which is applied to two states within the same sequence, as the foundation for sequence representations rather than an RNN. For the Arabic language, two transformer-based language models have been developed so far; notably AraBERT [31] and GigBAERT [32]. Both models aim at solving a masked language-modeling task in order to correctly predict a masked word from its context. Besides, these models aim at resolving a next sentence prediction task especially to decide whether two sentences are consecutive or not."
    },
    {
      "self_ref": "#/texts/42",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 623.08,
            "r": 266.56,
            "b": 633.184,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            23
          ]
        }
      ],
      "orig": "2.2. Domain-Specific MT",
      "text": "2.2. Domain-Specific MT",
      "level": 1
    },
    {
      "self_ref": "#/texts/43",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 3,
          "bbox": {
            "l": 165.41000000000003,
            "t": 638.236,
            "r": 560.49,
            "b": 774.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            954
          ]
        }
      ],
      "orig": "Domain translation is a challenging task due to the fact that language varies across different domains, genres, and styles. For example, texts in the financial domain often contain specific terminologies and jargon that may not be extensively used in legal or health domains. Therefore, researchers have proposed different methods to improve the quality of translations in domains such as medical and biomedical [7,33,34], legal [35], and financial texts [36]. Several domain adaptation approaches have been proposed (for more comprehensive survey see [3]). Domain adaptation methods can intervene in various stages of NMT system design, training and use can be classified into three main categories: data-centric methods, architecture-centric adaptation methods, and inference schemes for adaptation [3]. In data-centric methods, the objective is to select or generate appropriate in-domain data. Large generic monolingual data can be filtered to select",
      "text": "Domain translation is a challenging task due to the fact that language varies across different domains, genres, and styles. For example, texts in the financial domain often contain specific terminologies and jargon that may not be extensively used in legal or health domains. Therefore, researchers have proposed different methods to improve the quality of translations in domains such as medical and biomedical [7,33,34], legal [35], and financial texts [36]. Several domain adaptation approaches have been proposed (for more comprehensive survey see [3]). Domain adaptation methods can intervene in various stages of NMT system design, training and use can be classified into three main categories: data-centric methods, architecture-centric adaptation methods, and inference schemes for adaptation [3]. In data-centric methods, the objective is to select or generate appropriate in-domain data. Large generic monolingual data can be filtered to select"
    },
    {
      "self_ref": "#/texts/44",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/45",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "4 of 15",
      "text": "4 of 15"
    },
    {
      "self_ref": "#/texts/46",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 560.49,
            "b": 186.924,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            652
          ]
        }
      ],
      "orig": "domain-representative datasets based on some unique characteristics of the target domain. However, selecting a small in-domain dataset may be more domain-relevant, but the impact of any deviation from the target domain will be magnified [37]. Another approach is to construct partially synthetic bilingual training corpora by forward- or back-translation. Ref. [38] observed that models trained exclusively on back translations can perform similarly to models trained on natural data. Recently, the use of pre-trained large language models (LLMs) to generate large amounts of synthetic data at very low cost has emerged to be an effective approach [7].",
      "text": "domain-representative datasets based on some unique characteristics of the target domain. However, selecting a small in-domain dataset may be more domain-relevant, but the impact of any deviation from the target domain will be magnified [37]. Another approach is to construct partially synthetic bilingual training corpora by forward- or back-translation. Ref. [38] observed that models trained exclusively on back translations can perform similarly to models trained on natural data. Recently, the use of pre-trained large language models (LLMs) to generate large amounts of synthetic data at very low cost has emerged to be an effective approach [7]."
    },
    {
      "self_ref": "#/texts/47",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 190.292,
            "r": 560.49,
            "b": 314.908,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            812
          ]
        }
      ],
      "orig": "Architecture-centric adaptation typically involves adding trainable parameters to pretrained models to avoid training models from scratch. A common approach is to fine-tune an existing well-performing NMT model on small in-domain data. Extensive fine-tuning can lead to catastrophic forgetting. Ref. [39] proposed mixed-fine tuning which involves two steps: (1) training an NMT model on out-of-domain data until convergence and then (2) fine-tuning the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling in the in-domain data) until convergence. Mixed-fine tuning approaches can be helpful to prevent two major issues notably overlooking the specificity of each domain [1] and forgetting previously learned knowledge when exposed to the new training examples as reported in [40].",
      "text": "Architecture-centric adaptation typically involves adding trainable parameters to pretrained models to avoid training models from scratch. A common approach is to fine-tune an existing well-performing NMT model on small in-domain data. Extensive fine-tuning can lead to catastrophic forgetting. Ref. [39] proposed mixed-fine tuning which involves two steps: (1) training an NMT model on out-of-domain data until convergence and then (2) fine-tuning the NMT model from step 1 on a mix of in-domain and out-of-domain data (by oversampling in the in-domain data) until convergence. Mixed-fine tuning approaches can be helpful to prevent two major issues notably overlooking the specificity of each domain [1] and forgetting previously learned knowledge when exposed to the new training examples as reported in [40]."
    },
    {
      "self_ref": "#/texts/48",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 316.592,
            "r": 559.3,
            "b": 340.168,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            125
          ]
        }
      ],
      "orig": "Lastly, the inference schemes for adaptation develop a separate NMT model for each domain and combine them at inference time.",
      "text": "Lastly, the inference schemes for adaptation develop a separate NMT model for each domain and combine them at inference time."
    },
    {
      "self_ref": "#/texts/49",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 350.272,
            "r": 326.06,
            "b": 360.376,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            35
          ]
        }
      ],
      "orig": "2.3. Domain-Adaptation in Arabic MT",
      "text": "2.3. Domain-Adaptation in Arabic MT",
      "level": 1
    },
    {
      "self_ref": "#/texts/50",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 365.428,
            "r": 560.49,
            "b": 414.264,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            276
          ]
        }
      ],
      "orig": "The development of Arabic MT systems has gone through different stages, including rule-based systems [41,42], statistical MT [43], and more recently neural MT systems [44]. Ref. [45] conducted a comprehensive survey of Arabic MT systems and the unique challenges in Arabic MT.",
      "text": "The development of Arabic MT systems has gone through different stages, including rule-based systems [41,42], statistical MT [43], and more recently neural MT systems [44]. Ref. [45] conducted a comprehensive survey of Arabic MT systems and the unique challenges in Arabic MT."
    },
    {
      "self_ref": "#/texts/51",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 415.948,
            "r": 560.49,
            "b": 575.928,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1098
          ]
        }
      ],
      "orig": "Arabic is one of the official six languages adopted by the United Nations and it is spoken by 400 million people in the Middle East, North Africa, and many other parts of the world. Arabic is a Semitic language and it is notoriously difficult for MT due to its linguistic characteristics [45,46]. First, Arabic has a rich and complex morphology which is substantially different from English or other western languages [47]. Second, Arabic has long and short vowels. While the long vowels are represented by letters, the short vowels are marked by diacritic signs placed above or below the letters. However, the use of diacritic signs is not compulsory in Arabic and hence they are rarely used in informal writing. Therefore, it is hard to identify the correct sense of a word, especially when sufficient context is not provided. Third, variation among different Arabic dialects has always been problematic for AMT. Furthermore, the Arabic language used in social media varies considerably from Modern Standard Arabic (MSA). These aspects of the Arabic language pose series challenges for Arabic MT.",
      "text": "Arabic is one of the official six languages adopted by the United Nations and it is spoken by 400 million people in the Middle East, North Africa, and many other parts of the world. Arabic is a Semitic language and it is notoriously difficult for MT due to its linguistic characteristics [45,46]. First, Arabic has a rich and complex morphology which is substantially different from English or other western languages [47]. Second, Arabic has long and short vowels. While the long vowels are represented by letters, the short vowels are marked by diacritic signs placed above or below the letters. However, the use of diacritic signs is not compulsory in Arabic and hence they are rarely used in informal writing. Therefore, it is hard to identify the correct sense of a word, especially when sufficient context is not provided. Third, variation among different Arabic dialects has always been problematic for AMT. Furthermore, the Arabic language used in social media varies considerably from Modern Standard Arabic (MSA). These aspects of the Arabic language pose series challenges for Arabic MT."
    },
    {
      "self_ref": "#/texts/52",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 577.6120000000001,
            "r": 560.49,
            "b": 665.1800000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            560
          ]
        }
      ],
      "orig": "In addition to the aforementioned issues, there is a lack of high-quality parallel corpora of sufficient size for training or fine-tuning Arabic MT for different domains. It is commonly known that NMT systems do not perform well in domain-specific translation, especially in low-resource languages [1]. Addressing these challenges, some researchers have turned to domain-adaptation methods to develop domain-specific Arabic MT systems. For example, ref. [7] proposed the use of pre-trained LMs and back-translation for domain-specific data augmentation for MT.",
      "text": "In addition to the aforementioned issues, there is a lack of high-quality parallel corpora of sufficient size for training or fine-tuning Arabic MT for different domains. It is commonly known that NMT systems do not perform well in domain-specific translation, especially in low-resource languages [1]. Addressing these challenges, some researchers have turned to domain-adaptation methods to develop domain-specific Arabic MT systems. For example, ref. [7] proposed the use of pre-trained LMs and back-translation for domain-specific data augmentation for MT."
    },
    {
      "self_ref": "#/texts/53",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 4,
          "bbox": {
            "l": 165.41000000000003,
            "t": 668.548,
            "r": 560.49,
            "b": 752.748,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            548
          ]
        }
      ],
      "orig": "Furthermore, current Arabic MT research has primarily focused on the translation of limited domains such as news and official texts, whilst few attempts focus on domainspecific translation such as medical domain [48]. Specifically, most of the parallel data available to the researcher was limited to texts produced by international organizations, and parliamentary debates [33]. Unfortunately, existing single-domain AMT methods do not work well for multiple domains. Thus, multi-domain NMT approaches are more in demand to tackle this limitation.",
      "text": "Furthermore, current Arabic MT research has primarily focused on the translation of limited domains such as news and official texts, whilst few attempts focus on domainspecific translation such as medical domain [48]. Specifically, most of the parallel data available to the researcher was limited to texts produced by international organizations, and parliamentary debates [33]. Unfortunately, existing single-domain AMT methods do not work well for multiple domains. Thus, multi-domain NMT approaches are more in demand to tackle this limitation."
    },
    {
      "self_ref": "#/texts/54",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/55",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "5 of 15",
      "text": "5 of 15"
    },
    {
      "self_ref": "#/texts/56",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 560.49,
            "b": 212.184,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            808
          ]
        }
      ],
      "orig": "To recap, previous research has shown that domain adaptation leads to better translation quality than general NMT. While there has been considerable progress in general MT from Arabic to English [5,49], less work has been conducted on adapting models to specific domains such as medical domains [7], but to the best of our knowledge no work investigated the adaptability in financial texts. Since there is relatively little work on Arabic domain adaptation, the primary objective of this research is to explore the different effectiveness of domain translation methods, a yet unexplored domain, financial domain. To this end, this work aims to fine-tune several Transformer NMT models and LLM and perform cross-domain testing and evaluation to gain some insights into model robustness against domain changes.",
      "text": "To recap, previous research has shown that domain adaptation leads to better translation quality than general NMT. While there has been considerable progress in general MT from Arabic to English [5,49], less work has been conducted on adapting models to specific domains such as medical domains [7], but to the best of our knowledge no work investigated the adaptability in financial texts. Since there is relatively little work on Arabic domain adaptation, the primary objective of this research is to explore the different effectiveness of domain translation methods, a yet unexplored domain, financial domain. To this end, this work aims to fine-tune several Transformer NMT models and LLM and perform cross-domain testing and evaluation to gain some insights into model robustness against domain changes."
    },
    {
      "self_ref": "#/texts/57",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 225.656,
            "r": 238.0,
            "b": 237.444,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "3. Methodology",
      "text": "3. Methodology",
      "level": 1
    },
    {
      "self_ref": "#/texts/58",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 239.128,
            "r": 559.3,
            "b": 289.64799999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            313
          ]
        }
      ],
      "orig": "This section gives an overview of the methods and algorithms for AMT domain adaptation using LLM models. First, information about the collected bilingual dataset used is given which we refer to as the authentic dataset, then our approach is presented, and lastly, the metrics we used for evaluation are described.",
      "text": "This section gives an overview of the methods and algorithms for AMT domain adaptation using LLM models. First, information about the collected bilingual dataset used is given which we refer to as the authentic dataset, then our approach is presented, and lastly, the metrics we used for evaluation are described."
    },
    {
      "self_ref": "#/texts/59",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 301.436,
            "r": 222.53,
            "b": 311.54,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            13
          ]
        }
      ],
      "orig": "3.1. Approach",
      "text": "3.1. Approach",
      "level": 1
    },
    {
      "self_ref": "#/texts/60",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 316.592,
            "r": 560.49,
            "b": 390.68800000000005,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            472
          ]
        }
      ],
      "orig": "In this work, we investigate mainly two methods to augment our in-domain data for the domain of financial news and propose approaches to leverage pre-trained LLMs for domain-specific data generation for this MT task. Concerning domain-specific data generation, we start with synthetic data generation to augment our authentic sentences for Arabic. Then, to obtain the parallel data in English, we apply forward translation from the Arabic synthetic sentences into English.",
      "text": "In this work, we investigate mainly two methods to augment our in-domain data for the domain of financial news and propose approaches to leverage pre-trained LLMs for domain-specific data generation for this MT task. Concerning domain-specific data generation, we start with synthetic data generation to augment our authentic sentences for Arabic. Then, to obtain the parallel data in English, we apply forward translation from the Arabic synthetic sentences into English."
    },
    {
      "self_ref": "#/texts/61",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 400.792,
            "r": 305.83,
            "b": 410.896,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            32
          ]
        }
      ],
      "orig": "3.1.1. Synthetic Data Generation",
      "text": "3.1.1. Synthetic Data Generation",
      "level": 1
    },
    {
      "self_ref": "#/texts/62",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 415.948,
            "r": 560.49,
            "b": 666.864,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1643
          ]
        }
      ],
      "orig": "Synthetic data generation for data augmentation has been used in domain translation due to the scarcity of domain-specific datasets that are suitable for training large models. Ref. [7] proposed the use of state-of-the-art large language models to general unlimited new sentences in the source language and then back-translating in the target language. Recent studies explored the use of ChatGPT for generating new parallel sentences. However, in this study [50], the authors showed that the performance of ChatGPT for Arabic shows inferior performance compared to the finetuned AraT5. In our case, we leverage a pipeline of different models. We start with AraGPT2 [51] and gpt2 [52] as text generation models for Arabic and English to create synthetic pairs for (AR-EN) and (EN-AR), respectively. For Arabic, we use titles only from the collected authentic dataset as text prompts to generate corresponding long-form text using AraGPT2 hosted on HuggingFace (https://huggingface.ac/ombindlab/aragpt2-base, accessed on 13 March 2024). Then, we use mT5 [33] a summarizing model hosted on HuggingFace (https://huggingface.co/mtsm-bit-base-aar, accessed on 20 March 2024) to summarize the generated bunches of texts to obtain short summaries that will serve as generated titles. After that, we apply forward translation from Arabic to English using the OPS-MT model published on HuggingFace (https://huggingface.co/Helsinki-NLP/opus-mt-ar-en, accessed on 20 March 2024). Figure 1 shows the case of augmenting the authentic dataset with AR-EN synthetic pairs using this method. The same pipeline applies to English to obtain EN-AR synthetic pairs.",
      "text": "Synthetic data generation for data augmentation has been used in domain translation due to the scarcity of domain-specific datasets that are suitable for training large models. Ref. [7] proposed the use of state-of-the-art large language models to general unlimited new sentences in the source language and then back-translating in the target language. Recent studies explored the use of ChatGPT for generating new parallel sentences. However, in this study [50], the authors showed that the performance of ChatGPT for Arabic shows inferior performance compared to the finetuned AraT5. In our case, we leverage a pipeline of different models. We start with AraGPT2 [51] and gpt2 [52] as text generation models for Arabic and English to create synthetic pairs for (AR-EN) and (EN-AR), respectively. For Arabic, we use titles only from the collected authentic dataset as text prompts to generate corresponding long-form text using AraGPT2 hosted on HuggingFace (https://huggingface.ac/ombindlab/aragpt2-base, accessed on 13 March 2024). Then, we use mT5 [33] a summarizing model hosted on HuggingFace (https://huggingface.co/mtsm-bit-base-aar, accessed on 20 March 2024) to summarize the generated bunches of texts to obtain short summaries that will serve as generated titles. After that, we apply forward translation from Arabic to English using the OPS-MT model published on HuggingFace (https://huggingface.co/Helsinki-NLP/opus-mt-ar-en, accessed on 20 March 2024). Figure 1 shows the case of augmenting the authentic dataset with AR-EN synthetic pairs using this method. The same pipeline applies to English to obtain EN-AR synthetic pairs."
    },
    {
      "self_ref": "#/texts/63",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 165.41000000000003,
            "t": 767.904,
            "r": 559.3,
            "b": 778.008,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            104
          ]
        }
      ],
      "orig": "Figure 1. Data augmentation pipeline using (a) Arabic text generation and (b) Arabic text summarization.",
      "text": "Figure 1. Data augmentation pipeline using (a) Arabic text generation and (b) Arabic text summarization."
    },
    {
      "self_ref": "#/texts/64",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/65",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "6 of 15",
      "text": "6 of 15"
    },
    {
      "self_ref": "#/texts/66",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 266.56,
            "b": 101.03999999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            23
          ]
        }
      ],
      "orig": "3.1.2. Back-Translation",
      "text": "3.1.2. Back-Translation",
      "level": 1
    },
    {
      "self_ref": "#/texts/67",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 106.092,
            "r": 560.49,
            "b": 229.02400000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            818
          ]
        }
      ],
      "orig": "A common approach to augment domain data is the use of back-translation when there is abundant data in the target domain [1,38]. We use a pre-trained machine translation model [54] published on HuggingFace ( https://huggingface.co/Helsinki-NLP/opus-met-ar, accessed on 20 July 2024) for back-translation. The back-translation is applied on both sides of generated summaries and titles, namely on the long-form text (which serves as an article) as well as on the summarized form (which serves as a title) into the respective target language. In the end, we pair the generated summaries as well as the long-form text to serve as the title and article, respectively. The same pipeline applies to English as the target language. Figure 2 shows the case of augmenting the authentic dataset with EN-AR back-translated pairs.",
      "text": "A common approach to augment domain data is the use of back-translation when there is abundant data in the target domain [1,38]. We use a pre-trained machine translation model [54] published on HuggingFace ( https://huggingface.co/Helsinki-NLP/opus-met-ar, accessed on 20 July 2024) for back-translation. The back-translation is applied on both sides of generated summaries and titles, namely on the long-form text (which serves as an article) as well as on the summarized form (which serves as a title) into the respective target language. In the end, we pair the generated summaries as well as the long-form text to serve as the title and article, respectively. The same pipeline applies to English as the target language. Figure 2 shows the case of augmenting the authentic dataset with EN-AR back-translated pairs."
    },
    {
      "self_ref": "#/texts/68",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 341.85200000000003,
            "r": 468.86,
            "b": 353.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            74
          ]
        }
      ],
      "orig": "Figure 2. Data augmentation pipeline using back-translation from EN to AR.",
      "text": "Figure 2. Data augmentation pipeline using back-translation from EN to AR."
    },
    {
      "self_ref": "#/texts/69",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 367.112,
            "r": 254.66,
            "b": 378.90000000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "3.2. Experiment Setup",
      "text": "3.2. Experiment Setup",
      "level": 1
    },
    {
      "self_ref": "#/texts/70",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 382.26800000000003,
            "r": 229.67000000000002,
            "b": 392.372,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            15
          ]
        }
      ],
      "orig": "3.2.1. Datasets",
      "text": "3.2.1. Datasets",
      "level": 1
    },
    {
      "self_ref": "#/texts/71",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 399.108,
            "r": 560.49,
            "b": 486.676,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            561
          ]
        }
      ],
      "orig": "For fine-tuning in domain-specific MT models, we collected a dataset from different online resources for the pair AR-EN. As shown in Table 1 most of the data are collected from the Capital Markets Authority (CMA) yielding a total of 7560 AR-EN pairs. Note, that we consider titles (3780 AR-EN pairs) and articles (3780 AR-EN pairs). Additionally, we augmented our dataset with synthetic data as well as back-translated data. This step augmented the authentic dataset by 12,318 and 12,000 AR-EN sentence pairs as synthetic and back-translated data, respectively.",
      "text": "For fine-tuning in domain-specific MT models, we collected a dataset from different online resources for the pair AR-EN. As shown in Table 1 most of the data are collected from the Capital Markets Authority (CMA) yielding a total of 7560 AR-EN pairs. Note, that we consider titles (3780 AR-EN pairs) and articles (3780 AR-EN pairs). Additionally, we augmented our dataset with synthetic data as well as back-translated data. This step augmented the authentic dataset by 12,318 and 12,000 AR-EN sentence pairs as synthetic and back-translated data, respectively."
    },
    {
      "self_ref": "#/texts/72",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 486.676,
            "r": 560.49,
            "b": 548.984,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            373
          ]
        }
      ],
      "orig": "Table 2 shows the breakdown of the segments in our dataset. We randomly sampled 1000 segments from the authentic dataset to serve as test data for all models. Additionally, we randomly sampled 1000 segments for building the development for both models notably for OUP (ust-big) and NLLB. However, for fine-tuning ChatGPT, we randomly sampled 2000 pairs each for each setup.",
      "text": "Table 2 shows the breakdown of the segments in our dataset. We randomly sampled 1000 segments from the authentic dataset to serve as test data for all models. Additionally, we randomly sampled 1000 segments for building the development for both models notably for OUP (ust-big) and NLLB. However, for fine-tuning ChatGPT, we randomly sampled 2000 pairs each for each setup."
    },
    {
      "self_ref": "#/texts/73",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 560.772,
            "r": 308.21000000000004,
            "b": 570.8760000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            38
          ]
        }
      ],
      "orig": "Table 1. Authentic dataset statistics.",
      "text": "Table 1. Authentic dataset statistics."
    },
    {
      "self_ref": "#/texts/74",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 668.548,
            "r": 401.03000000000003,
            "b": 678.652,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            58
          ]
        }
      ],
      "orig": "Table 2. Authentic dataset split and augmented data count.",
      "text": "Table 2. Authentic dataset split and augmented data count."
    },
    {
      "self_ref": "#/texts/75",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/76",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "7 of 15",
      "text": "7 of 15"
    },
    {
      "self_ref": "#/texts/77",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 301.07,
            "b": 99.356,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            29
          ]
        }
      ],
      "orig": "3.2.2. Nmt Pre-Trained Models",
      "text": "3.2.2. Nmt Pre-Trained Models",
      "level": 1
    },
    {
      "self_ref": "#/texts/78",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 106.092,
            "r": 559.3,
            "b": 166.716,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            379
          ]
        }
      ],
      "orig": "Our generic NMT pre-trained models use different Transformer architectures; however, we have implemented the fine-tuning objective using the huggingface NMT transformer (a sequence-to-sequence version in the Transformers library) procedure. We adopt a common Seq2Seq architecture mainly composed of Encoder and Decoder network. The fine-tuning procedure is summarized as follows:",
      "text": "Our generic NMT pre-trained models use different Transformer architectures; however, we have implemented the fine-tuning objective using the huggingface NMT transformer (a sequence-to-sequence version in the Transformers library) procedure. We adopt a common Seq2Seq architecture mainly composed of Encoder and Decoder network. The fine-tuning procedure is summarized as follows:"
    },
    {
      "self_ref": "#/texts/79",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 171.768,
            "r": 454.58,
            "b": 183.556,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            61
          ]
        }
      ],
      "orig": "1. Load a pre-trained model with the corresponding Tokenizer.",
      "text": "1. Load a pre-trained model with the corresponding Tokenizer.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/80",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 185.24,
            "r": 559.3,
            "b": 208.816,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            159
          ]
        }
      ],
      "orig": "2. Tokenize the training, validation and test data into subwords (sentencePiece) so that training, validation and testing data will be truncated and tokenized.",
      "text": "2. Tokenize the training, validation and test data into subwords (sentencePiece) so that training, validation and testing data will be truncated and tokenized.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/81",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 210.5,
            "r": 560.49,
            "b": 257.652,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            284
          ]
        }
      ],
      "orig": "3. Instantiate the loaded model using Seq2Seq trainer (https://huggingface.co/docs/ transformers/en/main_classes/trainer#transformers.Seq2SeqTrainer, accessed on 1 June 2024) from huggingface while setting the AdamW (Adam with Weight Decay) optimizer with appropriate hyperparameters.",
      "text": "3. Instantiate the loaded model using Seq2Seq trainer (https://huggingface.co/docs/ transformers/en/main_classes/trainer#transformers.Seq2SeqTrainer, accessed on 1 June 2024) from huggingface while setting the AdamW (Adam with Weight Decay) optimizer with appropriate hyperparameters.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/82",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 259.336,
            "r": 559.3,
            "b": 282.91200000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            83
          ]
        }
      ],
      "orig": "4. Fine-tune the loaded model using backpropagation while minimizing cross-entropy.",
      "text": "4. Fine-tune the loaded model using backpropagation while minimizing cross-entropy.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/83",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 271.124,
            "r": 414.11999999999995,
            "b": 282.91200000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            52
          ]
        }
      ],
      "orig": "5. Save the fine-tuned model on the huggingFace hub.",
      "text": "5. Save the fine-tuned model on the huggingFace hub.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/84",
      "parent": {
        "$ref": "#/groups/2"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 284.596,
            "r": 559.3,
            "b": 308.17199999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            123
          ]
        }
      ],
      "orig": "6. Load the fine-tuned model for inference, feed the tokenized test data and decode the outputs to obtain the translations.",
      "text": "6. Load the fine-tuned model for inference, feed the tokenized test data and decode the outputs to obtain the translations.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/85",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 314.908,
            "r": 559.3,
            "b": 338.48400000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            165
          ]
        }
      ],
      "orig": "For Fine-tuning and inference, we use beam size 4 and batch size 16, on a GPU T4-15 GB (Google Colab). Further, we use ChatGPT as a baseline with zero-shot learning.",
      "text": "For Fine-tuning and inference, we use beam size 4 and batch size 16, on a GPU T4-15 GB (Google Colab). Further, we use ChatGPT as a baseline with zero-shot learning."
    },
    {
      "self_ref": "#/texts/86",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 340.168,
            "r": 560.49,
            "b": 414.264,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            469
          ]
        }
      ],
      "orig": "Opus (bt-big): We use OPUS [53] models from the Tatoeba-Challenge, specifically the models augmented with back-translated data of Wikimedia content and trained with Transformer-Big architecture. Here we picked the Helsinki-NLP/opus-mt-ar-en-checkpoint. For tokenization, we instantiate our tokenizer which is based on SentencePiece [56] with the AutoTokenizer.from_pretrained method. This ensures that the tokenizer corresponds to the model architecture we want to use.",
      "text": "Opus (bt-big): We use OPUS [53] models from the Tatoeba-Challenge, specifically the models augmented with back-translated data of Wikimedia content and trained with Transformer-Big architecture. Here we picked the Helsinki-NLP/opus-mt-ar-en-checkpoint. For tokenization, we instantiate our tokenizer which is based on SentencePiece [56] with the AutoTokenizer.from_pretrained method. This ensures that the tokenizer corresponds to the model architecture we want to use."
    },
    {
      "self_ref": "#/texts/87",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 415.948,
            "r": 560.49,
            "b": 490.044,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            453
          ]
        }
      ],
      "orig": "Nml: No-Language-Left-Bem (NBL [57]) is a multilingual model which supports 200 languages with a massive size Transformer. Fine-tuning is carried out on NLLB using its distilled version facebook/nllb-200-distilled-600M checkpoint. For tokenization, we instantiate a multilingual model provided by NLLB for tokenization with the NllbTokenizerFast.from_pretrained method. This ensures that the tokenizer corresponds to the model architecture we are using.",
      "text": "Nml: No-Language-Left-Bem (NBL [57]) is a multilingual model which supports 200 languages with a massive size Transformer. Fine-tuning is carried out on NLLB using its distilled version facebook/nllb-200-distilled-600M checkpoint. For tokenization, we instantiate a multilingual model provided by NLLB for tokenization with the NllbTokenizerFast.from_pretrained method. This ensures that the tokenizer corresponds to the model architecture we are using."
    },
    {
      "self_ref": "#/texts/88",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 491.72799999999995,
            "r": 560.49,
            "b": 628.132,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            887
          ]
        }
      ],
      "orig": "ChatGPT3.5: We use the ChatGPT-3.5-turbo model via its official API (https://chat. openai.com, accessed on 20 August 2023) which powers ChatGPT. Here, we prepare our dataset in the format that is accepted by the API functions. In particular, we convert the AR-EN pairs into the Prompt template for sentence-level translation as recommended in the OpenAI playground for sentence-level translation tasks. In order to avoid errors, we truncate all the sentence pairs to a max size of 4290 characters before sending the request. Moreover, we set the size of the total tokens to about 378,460 tokens due to limit rate costs. For this model, we formatted the requests with the system message first 'You are a professional translator in the financial domain. Translate the following Arabic sentence; ar_en into English' followed by user content messages, where ar_en represents the AR-EN pairs.",
      "text": "ChatGPT3.5: We use the ChatGPT-3.5-turbo model via its official API (https://chat. openai.com, accessed on 20 August 2023) which powers ChatGPT. Here, we prepare our dataset in the format that is accepted by the API functions. In particular, we convert the AR-EN pairs into the Prompt template for sentence-level translation as recommended in the OpenAI playground for sentence-level translation tasks. In order to avoid errors, we truncate all the sentence pairs to a max size of 4290 characters before sending the request. Moreover, we set the size of the total tokens to about 378,460 tokens due to limit rate costs. For this model, we formatted the requests with the system message first 'You are a professional translator in the financial domain. Translate the following Arabic sentence; ar_en into English' followed by user content messages, where ar_en represents the AR-EN pairs."
    },
    {
      "self_ref": "#/texts/89",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 639.92,
            "r": 230.86,
            "b": 651.708,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            15
          ]
        }
      ],
      "orig": "3.2.3. Settings",
      "text": "3.2.3. Settings",
      "level": 1
    },
    {
      "self_ref": "#/texts/90",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 656.76,
            "r": 559.3,
            "b": 692.1239999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            186
          ]
        }
      ],
      "orig": "Before starting the experiments, we considered the following three setups for finetuning the models on the domain-specific dataset. Next, Section 4 will discuss the results and findings.",
      "text": "Before starting the experiments, we considered the following three setups for finetuning the models on the domain-specific dataset. Next, Section 4 will discuss the results and findings."
    },
    {
      "self_ref": "#/texts/91",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 7,
          "bbox": {
            "l": 165.41000000000003,
            "t": 693.808,
            "r": 560.49,
            "b": 754.432,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            410
          ]
        }
      ],
      "orig": "Setup 1 (baseline models): We consider pre-trained NMT models evaluated on our cleaned authentic test split containing 1000 AR-EN sentence pairs. Our baseline NMT models use the OPUS (bt-big) (https://github.com/Helsinki-NLP/Tatoeba-Challenge/ tree/master/models/ara-eng, accessed on 12 March 2024) [54], NLLB 600 M (https:// huggingface.co/facebook/nllb-200-distilled-600M, accessed on 12 March 2024) [57] and",
      "text": "Setup 1 (baseline models): We consider pre-trained NMT models evaluated on our cleaned authentic test split containing 1000 AR-EN sentence pairs. Our baseline NMT models use the OPUS (bt-big) (https://github.com/Helsinki-NLP/Tatoeba-Challenge/ tree/master/models/ara-eng, accessed on 12 March 2024) [54], NLLB 600 M (https:// huggingface.co/facebook/nllb-200-distilled-600M, accessed on 12 March 2024) [57] and"
    },
    {
      "self_ref": "#/texts/92",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/93",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "8 of 15",
      "text": "8 of 15"
    },
    {
      "self_ref": "#/texts/94",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 559.3,
            "b": 111.144,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            106
          ]
        }
      ],
      "orig": "ChatGPT-3.5 (https://platform.openai.com/docs/guides/gpt/chat-completions-api, accessed on 12 March 2024).",
      "text": "ChatGPT-3.5 (https://platform.openai.com/docs/guides/gpt/chat-completions-api, accessed on 12 March 2024)."
    },
    {
      "self_ref": "#/texts/95",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 114.51200000000001,
            "r": 560.49,
            "b": 188.608,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            465
          ]
        }
      ],
      "orig": "Setup 2 (fine-tuning with authentic data): For fine-tuning, we have initialized the transformer models with the trained weights of the baselines. We use our authentic dataset with the splits shown in Table 2. We have kept all the hyperparameters identical. The models have been fine-tuned until convergence over the validation set. At test time, the respective test set from the authentic dataset is used for this setup as well. Again, all the metrics are reported.",
      "text": "Setup 2 (fine-tuning with authentic data): For fine-tuning, we have initialized the transformer models with the trained weights of the baselines. We use our authentic dataset with the splits shown in Table 2. We have kept all the hyperparameters identical. The models have been fine-tuned until convergence over the validation set. At test time, the respective test set from the authentic dataset is used for this setup as well. Again, all the metrics are reported."
    },
    {
      "self_ref": "#/texts/96",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 190.292,
            "r": 559.3,
            "b": 287.964,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            680
          ]
        }
      ],
      "orig": "Setup 3 (fine-tuning with augmented data): Similar to the previous setup, we have initialized the transformer models with the trained weights of the baselines. However, here we use our authentic dataset augmented with the respective data with the splits shown in Table 2. Basically, we augment the authentic dataset with back-translated data and shuffle it. The same applies to synthetic data. This step yields two versions of fine-tuning, one using the former and one using the latter. The models have been fine-tuned until convergence over the validation set. At test time, the test set from the authentic dataset is used for this setup as well while also reporting all metrics.",
      "text": "Setup 3 (fine-tuning with augmented data): Similar to the previous setup, we have initialized the transformer models with the trained weights of the baselines. However, here we use our authentic dataset augmented with the respective data with the splits shown in Table 2. Basically, we augment the authentic dataset with back-translated data and shuffle it. The same applies to synthetic data. This step yields two versions of fine-tuning, one using the former and one using the latter. The models have been fine-tuned until convergence over the validation set. At test time, the test set from the authentic dataset is used for this setup as well while also reporting all metrics."
    },
    {
      "self_ref": "#/texts/97",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 299.752,
            "r": 215.39,
            "b": 309.856,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            12
          ]
        }
      ],
      "orig": "3.3. Metrics",
      "text": "3.3. Metrics",
      "level": 1
    },
    {
      "self_ref": "#/texts/98",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 316.592,
            "r": 560.49,
            "b": 452.99600000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            938
          ]
        }
      ],
      "orig": "As performance measures, we report the spBLEU score [15] which uses a SentencePiece tokenizer, chrF [58], TER [59], and are implemented in sacrebleu [https://github.com/ mipost/sacreBLEU, accessed on 2 June 2023). Additionally, we compute COMET [60] that was proposed recently by taking advantage of cross-lingual pre-trained LMs using knowledge from both source and target languages. COMET makes a prediction score that correlates with human judgment [60]. For our experiments, we adopt the official COMET implementation (https://github.com/Unbabel/COMET, accessed on 2 June 2023). For COMET, we use the reference-based Estimation model wmt20-comet-da, trained based on Direct Assessment (DA) and used Quality Estimation (QE). Another score that correlates with human evaluation BERTScore [61] is also computed. Including different metrics in the evaluation allows us to test the models on metrics different from those used for training.",
      "text": "As performance measures, we report the spBLEU score [15] which uses a SentencePiece tokenizer, chrF [58], TER [59], and are implemented in sacrebleu [https://github.com/ mipost/sacreBLEU, accessed on 2 June 2023). Additionally, we compute COMET [60] that was proposed recently by taking advantage of cross-lingual pre-trained LMs using knowledge from both source and target languages. COMET makes a prediction score that correlates with human judgment [60]. For our experiments, we adopt the official COMET implementation (https://github.com/Unbabel/COMET, accessed on 2 June 2023). For COMET, we use the reference-based Estimation model wmt20-comet-da, trained based on Direct Assessment (DA) and used Quality Estimation (QE). Another score that correlates with human evaluation BERTScore [61] is also computed. Including different metrics in the evaluation allows us to test the models on metrics different from those used for training."
    },
    {
      "self_ref": "#/texts/99",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 463.1,
            "r": 283.21999999999997,
            "b": 473.20400000000006,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            25
          ]
        }
      ],
      "orig": "4. Results and Discussion",
      "text": "4. Results and Discussion",
      "level": 1
    },
    {
      "self_ref": "#/texts/100",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 479.93999999999994,
            "r": 560.49,
            "b": 589.4,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            767
          ]
        }
      ],
      "orig": "This section elaborates on our automatic and human evaluations and discusses the results. We also provide a preliminary comparison of the models' performance on domainspecific MT as baseline models and as fine-tuned models. Therefore, we report if they can perform robustly well on domain-specific or even noisy sentences from our collected dataset. Specifically, we focus on the translation robustness of the models in the translation of Arabic financial news. Table 3 shows the main results of the respective testset. The ↑ and ↓ symbols in the tables indicate which values are better. We analyze the translation outputs by comparing the MT evaluation metrics in each setup. Visual plots of the models' performances in each of the setup are presented in Appendix A.",
      "text": "This section elaborates on our automatic and human evaluations and discusses the results. We also provide a preliminary comparison of the models' performance on domainspecific MT as baseline models and as fine-tuned models. Therefore, we report if they can perform robustly well on domain-specific or even noisy sentences from our collected dataset. Specifically, we focus on the translation robustness of the models in the translation of Arabic financial news. Table 3 shows the main results of the respective testset. The ↑ and ↓ symbols in the tables indicate which values are better. We analyze the translation outputs by comparing the MT evaluation metrics in each setup. Visual plots of the models' performances in each of the setup are presented in Appendix A."
    },
    {
      "self_ref": "#/texts/101",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 165.41000000000003,
            "t": 604.5559999999999,
            "r": 559.3,
            "b": 628.132,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            116
          ]
        }
      ],
      "orig": "Table 3. MT evaluation scores and human evaluation for AR-EN Test dataset (1000 pairs). The best scores are in bold.",
      "text": "Table 3. MT evaluation scores and human evaluation for AR-EN Test dataset (1000 pairs). The best scores are in bold."
    },
    {
      "self_ref": "#/texts/102",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/103",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 535.5,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            7
          ]
        }
      ],
      "orig": "9 of 15",
      "text": "9 of 15"
    },
    {
      "self_ref": "#/texts/104",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "caption",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 166.60000000000002,
            "t": 89.252,
            "r": 218.96,
            "b": 99.356,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "Table 3. Cont.",
      "text": "Table 3. Cont."
    },
    {
      "self_ref": "#/texts/105",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 165.41000000000003,
            "t": 210.5,
            "r": 560.49,
            "b": 308.17199999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1290
          ]
        }
      ],
      "orig": "BMT = BRT, FT-Fine-Tuned, S = synthetic, the direction of the arrows indicate better performance 1 https: //huggingface.co/HelsNiklin-P/opus-mt-ar-en/accessed on 2 June 2024; 2) https: //huggingface.co/asas-an/lib-to-accessed 200-distilled-600M (accessed on 2 June 2024; 3) https: //huggingface.co/asas-an/lib-to-accessed-on 2 June 2024; 4) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 5) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 6) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 7) https: //huggingface.co/asas-an/lib-20-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 8) https: //huggingface.co/asas-an/lib-to-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 9) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 10) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 11) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 12) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 13)",
      "text": "BMT = BRT, FT-Fine-Tuned, S = synthetic, the direction of the arrows indicate better performance 1 https: //huggingface.co/HelsNiklin-P/opus-mt-ar-en/accessed on 2 June 2024; 2) https: //huggingface.co/asas-an/lib-to-accessed 200-distilled-600M (accessed on 2 June 2024; 3) https: //huggingface.co/asas-an/lib-to-accessed-on 2 June 2024; 4) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 5) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 6) https: //huggingface.co/asas-an/lib-to-accessed-on-17-art-en-accessed-on 2 June 2024; 7) https: //huggingface.co/asas-an/lib-20-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 8) https: //huggingface.co/asas-an/lib-to-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 9) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 10) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 11) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 12) https: //huggingface.co/asas-an/lib-200-distilled-600M-finetuned-augmented_MT-ar-to-en (accessed on 2 June 2024; 13)"
    },
    {
      "self_ref": "#/texts/106",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 165.41000000000003,
            "t": 321.644,
            "r": 273.7,
            "b": 331.748,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            25
          ]
        }
      ],
      "orig": "4.1. Automatic Evaluation",
      "text": "4.1. Automatic Evaluation",
      "level": 1
    },
    {
      "self_ref": "#/texts/107",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 165.41000000000003,
            "t": 336.8,
            "r": 559.3,
            "b": 522.04,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1268
          ]
        }
      ],
      "orig": "In Setup 1, OPUS and NLLB perform equally with inferior performances of around 14 and 42 for BLEU and chrF points, respectively. The TER score which is expressed as the ratio of the number of edits to the average number of words in the reference is high for the two models. Thus, it indicates that the translation is of poor quality. In terms of COMET score, both models have very poor results which means reference-based COMET may lose information from source, translation output, or reference embeddings, except for ChatGPT-3.5. But, BERTScores for all three models are high which means that they do not correlate with COMET score. In comparison, BERTScore and COMET have a significant difference in their scores. In contrast, ChatGPT-3.5 performs competitively better (BLEU 26.13) than OPUS and NLLB models. Indeed, we are not surprised by this fact which is in line with related research works [50,62,63]. However, these findings are not consistent with a previous finding [64] where the authors evaluated ChatGPT and on 4000 ArabicEnglish pairs and found out that SoTA models like araT5 [44] outperforms ChatGPT by 19 BLEU Points. Similarly, ref. [65] found the English-to-Arabic translation of ChatGPT-3.5 was below average compared to 14 established MT systems.",
      "text": "In Setup 1, OPUS and NLLB perform equally with inferior performances of around 14 and 42 for BLEU and chrF points, respectively. The TER score which is expressed as the ratio of the number of edits to the average number of words in the reference is high for the two models. Thus, it indicates that the translation is of poor quality. In terms of COMET score, both models have very poor results which means reference-based COMET may lose information from source, translation output, or reference embeddings, except for ChatGPT-3.5. But, BERTScores for all three models are high which means that they do not correlate with COMET score. In comparison, BERTScore and COMET have a significant difference in their scores. In contrast, ChatGPT-3.5 performs competitively better (BLEU 26.13) than OPUS and NLLB models. Indeed, we are not surprised by this fact which is in line with related research works [50,62,63]. However, these findings are not consistent with a previous finding [64] where the authors evaluated ChatGPT and on 4000 ArabicEnglish pairs and found out that SoTA models like araT5 [44] outperforms ChatGPT by 19 BLEU Points. Similarly, ref. [65] found the English-to-Arabic translation of ChatGPT-3.5 was below average compared to 14 established MT systems."
    },
    {
      "self_ref": "#/texts/108",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 165.41000000000003,
            "t": 523.724,
            "r": 560.49,
            "b": 651.708,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            831
          ]
        }
      ],
      "orig": "When, we analyze Setup 2, as expected, fine-tuning all models on authentic data has generally helped improve the BLEU scores and other metrics as well. This finding is also inline with other previous research [7,56]. However, ref. [62] noticed that for domain-specific translation (e.g., in the biomedical field), ChatGPT's performance degrades considerably. We attribute this behavior to the observation that ChatGPT is capable of translating our sentences better than terminologies in sentences from the biomedical domain, a very specific domain. Furthermore, we clearly, see that BLEU scores increase from 14.58 to 48.83, from 14.38 to 43.43 and from 26.13 to 51.15 for OPUS, NLLB and ChatGPT-3.5, respectively. In terms of COMET and BERTScore, both metrics had a high correlation which indicates acceptable translation outputs.",
      "text": "When, we analyze Setup 2, as expected, fine-tuning all models on authentic data has generally helped improve the BLEU scores and other metrics as well. This finding is also inline with other previous research [7,56]. However, ref. [62] noticed that for domain-specific translation (e.g., in the biomedical field), ChatGPT's performance degrades considerably. We attribute this behavior to the observation that ChatGPT is capable of translating our sentences better than terminologies in sentences from the biomedical domain, a very specific domain. Furthermore, we clearly, see that BLEU scores increase from 14.58 to 48.83, from 14.38 to 43.43 and from 26.13 to 51.15 for OPUS, NLLB and ChatGPT-3.5, respectively. In terms of COMET and BERTScore, both metrics had a high correlation which indicates acceptable translation outputs."
    },
    {
      "self_ref": "#/texts/109",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 165.41000000000003,
            "t": 653.392,
            "r": 560.49,
            "b": 774.64,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            879
          ]
        }
      ],
      "orig": "Concerning ChatGPT, even though it only used 2000 pairs of AR-EN sentences for fine-tuning, it outperforms all other models which means the MT quality of ChatGPT can easily be improved with little additional data from the language pair, a fact that has not been previously confirmed for related approaches, since this is the first work that assesses the performance of ChatGPT fine-tuned models for AR-EN MT task. Nevertheless, for English, this work [66] has shown that ChatGPT has great robust translation capabilities over related SoTA MT models. Our experimental result confirms the latter finding and shows that with a carefully prepared certain amount of fine-tuning data, this model is capable of creating acceptable translations. As for the translation robustness, results from Setup 2 suggest that ChatGPT-3.5 performs competitively well on financial news. Regarding the",
      "text": "Concerning ChatGPT, even though it only used 2000 pairs of AR-EN sentences for fine-tuning, it outperforms all other models which means the MT quality of ChatGPT can easily be improved with little additional data from the language pair, a fact that has not been previously confirmed for related approaches, since this is the first work that assesses the performance of ChatGPT fine-tuned models for AR-EN MT task. Nevertheless, for English, this work [66] has shown that ChatGPT has great robust translation capabilities over related SoTA MT models. Our experimental result confirms the latter finding and shows that with a carefully prepared certain amount of fine-tuning data, this model is capable of creating acceptable translations. As for the translation robustness, results from Setup 2 suggest that ChatGPT-3.5 performs competitively well on financial news. Regarding the"
    },
    {
      "self_ref": "#/texts/110",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/111",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "10 of 15",
      "text": "10 of 15"
    },
    {
      "self_ref": "#/texts/112",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 89.252,
            "r": 560.49,
            "b": 126.3,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            259
          ]
        }
      ],
      "orig": "human evaluation, all models in this setup reached possible and acceptable translations. We conclude that our experiment shows that providing in-domain examples to ChatGPT achieves comparable results to a SoTa model in terms of automatic and human evaluation.",
      "text": "human evaluation, all models in this setup reached possible and acceptable translations. We conclude that our experiment shows that providing in-domain examples to ChatGPT achieves comparable results to a SoTa model in terms of automatic and human evaluation."
    },
    {
      "self_ref": "#/texts/113",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 127.984,
            "r": 560.49,
            "b": 299.752,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1213
          ]
        }
      ],
      "orig": "In Setup 3 we fine-tune the baseline models with the augmented data in two versions, one using back-translated data and the other using synthetic data. We observe that both lexical metrics (BLEU and chRf) show consistent degradation with all models. The same applies to the TER score. For instance, for ChatGPT, the BLEU score decreased dramatically from 51.15 to 34.67 when fine-tuned on synthetic data while maintaining an acceptable score (BLEU 45.38) when fine-tuned on back-translated data. We observe that the COMET score degraded massively for ChatGPT more than for Opus and NLLB. One explanation could be that the synthetic data may have a lot of generated tokens that are grammatically correct, but they have nonsense meaning, as we know from the current state of the generative text. This could indicate that the translation results are not close in the embedding space with the source and reference. In contrast, BERTSCore maintained a good score over the two versions for all models. In setup, Opus (bT)fg (BT) fat (BT)M had it made the best model that provides reasonably good scores translations; however, it still lags behind the Opus model fine-tuned on authentic data by at least 1.3 BLEU points.",
      "text": "In Setup 3 we fine-tune the baseline models with the augmented data in two versions, one using back-translated data and the other using synthetic data. We observe that both lexical metrics (BLEU and chRf) show consistent degradation with all models. The same applies to the TER score. For instance, for ChatGPT, the BLEU score decreased dramatically from 51.15 to 34.67 when fine-tuned on synthetic data while maintaining an acceptable score (BLEU 45.38) when fine-tuned on back-translated data. We observe that the COMET score degraded massively for ChatGPT more than for Opus and NLLB. One explanation could be that the synthetic data may have a lot of generated tokens that are grammatically correct, but they have nonsense meaning, as we know from the current state of the generative text. This could indicate that the translation results are not close in the embedding space with the source and reference. In contrast, BERTSCore maintained a good score over the two versions for all models. In setup, Opus (bT)fg (BT) fat (BT)M had it made the best model that provides reasonably good scores translations; however, it still lags behind the Opus model fine-tuned on authentic data by at least 1.3 BLEU points."
    },
    {
      "self_ref": "#/texts/114",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 301.436,
            "r": 560.49,
            "b": 466.468,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            1103
          ]
        }
      ],
      "orig": "Generally, the drop in performance for all models in this setup is not consistent with others' research. For instance, the authors in [7] used synthetic data in the healthcare domain and achieved improvements on the in-domain test set. In comparison, with this work, the authors applied synthetic data generation using mGPT (https://huggingface.co/ sberbank-ai/mGPT, accessed on 12 March 2024) a multilingual language model. We argue that this model might have better perplexity in generated tokens compared to araGPT2. To the best of our knowledge, we did not find any research work investigating the performance of both models in regard to Arabic. We will further investigate this issue in future work. However, there are many general reasons explaining Opus, NLLB and ChatGPT behavior in domain-specific MT, especially in the case of augmenting the dataset with synthetic data. One explanation is that the use of synthetic data may cause incorrect token choices, grammatical errors, or unnatural sentence structures to propagate into the translation outputs which make suboptimal translation outputs.",
      "text": "Generally, the drop in performance for all models in this setup is not consistent with others' research. For instance, the authors in [7] used synthetic data in the healthcare domain and achieved improvements on the in-domain test set. In comparison, with this work, the authors applied synthetic data generation using mGPT (https://huggingface.co/ sberbank-ai/mGPT, accessed on 12 March 2024) a multilingual language model. We argue that this model might have better perplexity in generated tokens compared to araGPT2. To the best of our knowledge, we did not find any research work investigating the performance of both models in regard to Arabic. We will further investigate this issue in future work. However, there are many general reasons explaining Opus, NLLB and ChatGPT behavior in domain-specific MT, especially in the case of augmenting the dataset with synthetic data. One explanation is that the use of synthetic data may cause incorrect token choices, grammatical errors, or unnatural sentence structures to propagate into the translation outputs which make suboptimal translation outputs."
    },
    {
      "self_ref": "#/texts/115",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 468.15200000000004,
            "r": 560.49,
            "b": 575.928,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            740
          ]
        }
      ],
      "orig": "Indeed, the results of this study demonstrate the models' robust translation capabilities for in-domain adaptation. They perform well when fine-tuned on authentic data. However, we observe a discrepancy between COMET and BERTSCore. For instance, ChatGPT-3.5 performs worse on augmented data yielding a lower COMET score (23.03) but still having high BERTSCore (0.91). This behavior seems uncommon. One possible explanation is that COMET with reference-based translation is failing to find closeness in all three resource embeddings, whereas BERTSCore is able the find closeness in the similarity between an MT output and a reference translation. This behavior encourages us to drive human evaluation a much-needed score for trustworthiness.",
      "text": "Indeed, the results of this study demonstrate the models' robust translation capabilities for in-domain adaptation. They perform well when fine-tuned on authentic data. However, we observe a discrepancy between COMET and BERTSCore. For instance, ChatGPT-3.5 performs worse on augmented data yielding a lower COMET score (23.03) but still having high BERTSCore (0.91). This behavior seems uncommon. One possible explanation is that COMET with reference-based translation is failing to find closeness in all three resource embeddings, whereas BERTSCore is able the find closeness in the similarity between an MT output and a reference translation. This behavior encourages us to drive human evaluation a much-needed score for trustworthiness."
    },
    {
      "self_ref": "#/texts/116",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 587.716,
            "r": 260.61,
            "b": 599.504,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            21
          ]
        }
      ],
      "orig": "4.2. Human Evaluation",
      "text": "4.2. Human Evaluation",
      "level": 1
    },
    {
      "self_ref": "#/texts/117",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 604.5559999999999,
            "r": 560.49,
            "b": 740.96,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            948
          ]
        }
      ],
      "orig": "In addition to the automatic evaluations reported above, we decided to assess the quality of our models' translations using human evaluation. Machine translation metrics such as BLEU only measure the linguistic proximity of outputs to the gold standard of reference. On the other hand, COMET and BERTSCore which overcome this aforementioned issue, still exhibit discrepancy issues. To gain further insights into these results, we conducted a human evaluation. To this end, we recruited three native speakers and domain experts (post-graduate students in finance) to rate the acceptability of 50 randomly selected sentences from the test set. Similar to [7], we conducted a bilingual evaluation, whereby the evaluators rated both the original source sentences and translations generated by the MT models. The human evaluators were asked to rate each of the sentences based on the scale proposed by [67], ranging from 1 to 4, and outlined as follows:",
      "text": "In addition to the automatic evaluations reported above, we decided to assess the quality of our models' translations using human evaluation. Machine translation metrics such as BLEU only measure the linguistic proximity of outputs to the gold standard of reference. On the other hand, COMET and BERTSCore which overcome this aforementioned issue, still exhibit discrepancy issues. To gain further insights into these results, we conducted a human evaluation. To this end, we recruited three native speakers and domain experts (post-graduate students in finance) to rate the acceptability of 50 randomly selected sentences from the test set. Similar to [7], we conducted a bilingual evaluation, whereby the evaluators rated both the original source sentences and translations generated by the MT models. The human evaluators were asked to rate each of the sentences based on the scale proposed by [67], ranging from 1 to 4, and outlined as follows:"
    },
    {
      "self_ref": "#/texts/118",
      "parent": {
        "$ref": "#/groups/3"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 10,
          "bbox": {
            "l": 165.41000000000003,
            "t": 747.696,
            "r": 559.3,
            "b": 771.272,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            123
          ]
        }
      ],
      "orig": "· 4 = Ideal: Not necessarily a perfect translation, but grammatically correct, with all information accurately transferred.",
      "text": "· 4 = Ideal: Not necessarily a perfect translation, but grammatically correct, with all information accurately transferred.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/119",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/120",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "11 of 15",
      "text": "11 of 15"
    },
    {
      "self_ref": "#/texts/121",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 166.60000000000002,
            "t": 87.568,
            "r": 559.3,
            "b": 111.144,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            159
          ]
        }
      ],
      "orig": "· 3 = Acceptable: Not perfect (stylistically or grammatically odd), but definitely comprehensible, and with the accurate transfer of all important information.",
      "text": "· 3 = Acceptable: Not perfect (stylistically or grammatically odd), but definitely comprehensible, and with the accurate transfer of all important information.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/122",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 166.60000000000002,
            "t": 114.51200000000001,
            "r": 559.3,
            "b": 138.088,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            142
          ]
        }
      ],
      "orig": "· 2 = Possibly Acceptable: Possibly comprehensible (given enough context and/or time to work it out); some information transferred accurately.",
      "text": "· 2 = Possibly Acceptable: Possibly comprehensible (given enough context and/or time to work it out); some information transferred accurately.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/123",
      "parent": {
        "$ref": "#/groups/4"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 166.60000000000002,
            "t": 139.77200000000002,
            "r": 559.3,
            "b": 163.348,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            108
          ]
        }
      ],
      "orig": "· 1 = Unacceptable: Absolutely not comprehensible and/or little or no information is accurately transferred.",
      "text": "· 1 = Unacceptable: Absolutely not comprehensible and/or little or no information is accurately transferred.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/124",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 168.4,
            "r": 560.49,
            "b": 230.70800000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            433
          ]
        }
      ],
      "orig": "We first asked the three human evaluators to rate one model's output and then we conducted an inter-rater reliability analysis on their ratings. The result of weighted Cohen's Kappa is 0.87. Then, we asked each evaluator to rate the outputs of the remaining models and provide justification for their responses were \" Ideal\" or \" Unacceptable\" . The mean value of the raters' scores was averaged for each system, as shown in Table 3.",
      "text": "We first asked the three human evaluators to rate one model's output and then we conducted an inter-rater reliability analysis on their ratings. The result of weighted Cohen's Kappa is 0.87. Then, we asked each evaluator to rate the outputs of the remaining models and provide justification for their responses were \" Ideal\" or \" Unacceptable\" . The mean value of the raters' scores was averaged for each system, as shown in Table 3."
    },
    {
      "self_ref": "#/texts/125",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 232.39200000000002,
            "r": 560.49,
            "b": 267.75600000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            250
          ]
        }
      ],
      "orig": "Overall, the outcome of human evaluation corroborates the automatic evaluation results. In Setup 1 and 2, where ChatGPT-3.5 achieved the best performance in lexical and semantic metrics, human evaluation confirmed this result with a top score of 3.1.",
      "text": "Overall, the outcome of human evaluation corroborates the automatic evaluation results. In Setup 1 and 2, where ChatGPT-3.5 achieved the best performance in lexical and semantic metrics, human evaluation confirmed this result with a top score of 3.1."
    },
    {
      "self_ref": "#/texts/126",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 269.44,
            "r": 560.49,
            "b": 306.488,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            186
          ]
        }
      ],
      "orig": "However, in Setup 3, even though the automatic metrics are degraded for all models, except BERTScore, the human evaluation shows that the translation quality of all models is comparable.",
      "text": "However, in Setup 3, even though the automatic metrics are degraded for all models, except BERTScore, the human evaluation shows that the translation quality of all models is comparable."
    },
    {
      "self_ref": "#/texts/127",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 308.17199999999997,
            "r": 560.49,
            "b": 357.008,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            339
          ]
        }
      ],
      "orig": "Thus, we find that BERTScore correlates with human judgment more than COMET which has been recently reported to correlate highly with with human judgment [68]. This finding opens a great investigation for the future into whether semantic metrics correlate with human judgment and to what extent, in particular, when ChatGPT-3.5 is applied.",
      "text": "Thus, we find that BERTScore correlates with human judgment more than COMET which has been recently reported to correlate highly with with human judgment [68]. This finding opens a great investigation for the future into whether semantic metrics correlate with human judgment and to what extent, in particular, when ChatGPT-3.5 is applied."
    },
    {
      "self_ref": "#/texts/128",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 367.112,
            "r": 232.05,
            "b": 377.216,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            14
          ]
        }
      ],
      "orig": "5. Conclusions",
      "text": "5. Conclusions",
      "level": 1
    },
    {
      "self_ref": "#/texts/129",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 383.952,
            "r": 560.49,
            "b": 483.30799999999994,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            658
          ]
        }
      ],
      "orig": "In this paper, we conducted several experiments to assess the performance of pretrained NMT and LLM like GPT-3.5 using data augmentation in the domain of Arabic financial news articles. Generally, the results obtained from these experiments are very promising. While ChatGPT shows good results using few pairs, other models need more examples and still have lower performance. We explored the effectiveness of all models using data augmentation in the financial domain and found that MT quality decreased for all the models adequately. Here, ChatGPT shows inferior performance, while OPUS still performs better on back-translated data than on synthetic data.",
      "text": "In this paper, we conducted several experiments to assess the performance of pretrained NMT and LLM like GPT-3.5 using data augmentation in the domain of Arabic financial news articles. Generally, the results obtained from these experiments are very promising. While ChatGPT shows good results using few pairs, other models need more examples and still have lower performance. We explored the effectiveness of all models using data augmentation in the financial domain and found that MT quality decreased for all the models adequately. Here, ChatGPT shows inferior performance, while OPUS still performs better on back-translated data than on synthetic data."
    },
    {
      "self_ref": "#/texts/130",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 484.99199999999996,
            "r": 560.49,
            "b": 569.192,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            574
          ]
        }
      ],
      "orig": "There are many future works that can be carried out based on the findings from this study. Firstly, we would like to explore new techniques and methods to enhance translation outputs rather than the approach of data augmentation. Secondly, we think it is valuable to integrate more high-performance automatic metrics into the comparison that take semantics into consideration in a better way than in COMET and BERTScore. Finally, we will explore novel approaches to integrate additional models or even incorporate domain-specific models for improved translation performance.",
      "text": "There are many future works that can be carried out based on the findings from this study. Firstly, we would like to explore new techniques and methods to enhance translation outputs rather than the approach of data augmentation. Secondly, we think it is valuable to integrate more high-performance automatic metrics into the comparison that take semantics into consideration in a better way than in COMET and BERTScore. Finally, we will explore novel approaches to integrate additional models or even incorporate domain-specific models for improved translation performance."
    },
    {
      "self_ref": "#/texts/131",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 582.664,
            "r": 560.49,
            "b": 641.604,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            456
          ]
        }
      ],
      "orig": "Author Contributions: Conceptualization, E.A.A. and J.Z. methodology, E.A.A. and J.Z.; validation, E.A.A.J.Z. and F.A.A.; resources, A.A.; resources data, F.A.A.; writing-original draft preparation, E.A.A. and J.Z.; writing-review and editing, E.A.A.J.Z. and F.A.A.; visualization, J.Z. and F.A.A.; supervision, E.A.A.; project administration, E.A.A.; funding acquisition, E.A.A. All authors have read and agreed to the published version of the manuscript.",
      "text": "Author Contributions: Conceptualization, E.A.A. and J.Z. methodology, E.A.A. and J.Z.; validation, E.A.A.J.Z. and F.A.A.; resources, A.A.; resources data, F.A.A.; writing-original draft preparation, E.A.A. and J.Z.; writing-review and editing, E.A.A.J.Z. and F.A.A.; visualization, J.Z. and F.A.A.; supervision, E.A.A.; project administration, E.A.A.; funding acquisition, E.A.A. All authors have read and agreed to the published version of the manuscript."
    },
    {
      "self_ref": "#/texts/132",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 646.6560000000001,
            "r": 560.49,
            "b": 682.0200000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            305
          ]
        }
      ],
      "orig": "Funding: The research team obtained funding from the Translation Studies and Research Grants Program of the General Authority for Literature, Publishing and Translation at the Ministry of Culture in the Kingdom of Saudi Arabia to complete this research study in the field of translation for the year 2022.",
      "text": "Funding: The research team obtained funding from the Translation Studies and Research Grants Program of the General Authority for Literature, Publishing and Translation at the Ministry of Culture in the Kingdom of Saudi Arabia to complete this research study in the field of translation for the year 2022."
    },
    {
      "self_ref": "#/texts/133",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 688.756,
            "r": 560.49,
            "b": 722.436,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            217
          ]
        }
      ],
      "orig": "Institutional Review Board Statement: The human study was approved by the Ethics Committee at the English Language Institute in King Abdulaziz University (protocol code: EA23TR2-v1, date of approval: 5 February 2023).",
      "text": "Institutional Review Board Statement: The human study was approved by the Ethics Committee at the English Language Institute in King Abdulaziz University (protocol code: EA23TR2-v1, date of approval: 5 February 2023)."
    },
    {
      "self_ref": "#/texts/134",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 729.172,
            "r": 560.49,
            "b": 740.96,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            98
          ]
        }
      ],
      "orig": "Informed Consent Statement: Informed consent was obtained from all subjects involved in the study.",
      "text": "Informed Consent Statement: Informed consent was obtained from all subjects involved in the study."
    },
    {
      "self_ref": "#/texts/135",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 11,
          "bbox": {
            "l": 165.41000000000003,
            "t": 747.696,
            "r": 560.49,
            "b": 771.272,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            145
          ]
        }
      ],
      "orig": "Data Availability Statement: We made our datasets and fine-tuned models available at https:// huggingface.co/asas-ai/ (accessed on 19 July 2024).",
      "text": "Data Availability Statement: We made our datasets and fine-tuned models available at https:// huggingface.co/asas-ai/ (accessed on 19 July 2024)."
    },
    {
      "self_ref": "#/texts/136",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/137",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "12 of 15",
      "text": "12 of 15"
    },
    {
      "self_ref": "#/texts/138",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 166.60000000000002,
            "t": 89.252,
            "r": 559.3,
            "b": 126.3,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            199
          ]
        }
      ],
      "orig": "Acknowledgments: We would like to acknowledge the support from the Center of Excellence in AI and Data Science and the Center of Excellence of High-Performance Computing at King Abdulaziz University.",
      "text": "Acknowledgments: We would like to acknowledge the support from the Center of Excellence in AI and Data Science and the Center of Excellence of High-Performance Computing at King Abdulaziz University."
    },
    {
      "self_ref": "#/texts/139",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 166.60000000000002,
            "t": 131.352,
            "r": 559.3,
            "b": 178.504,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            297
          ]
        }
      ],
      "orig": "Conflicts of Interest: Authors Emad A. Alghamdi, Jezia Zakraoui and Fares A. Abanmy was employed by the company ASAS AI Lab. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.",
      "text": "Conflicts of Interest: Authors Emad A. Alghamdi, Jezia Zakraoui and Fares A. Abanmy was employed by the company ASAS AI Lab. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest."
    },
    {
      "self_ref": "#/texts/140",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 166.60000000000002,
            "t": 190.292,
            "r": 221.34,
            "b": 202.07999999999998,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            10
          ]
        }
      ],
      "orig": "Appendix A",
      "text": "Appendix A",
      "level": 1
    },
    {
      "self_ref": "#/texts/141",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 117.81,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/142",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "13 of 15",
      "text": "13 of 15"
    },
    {
      "self_ref": "#/texts/143",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "section_header",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 365.428,
            "r": 84.49,
            "b": 375.532,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            10
          ]
        }
      ],
      "orig": "References",
      "text": "References",
      "level": 1
    },
    {
      "self_ref": "#/texts/144",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 380.584,
            "r": 447.44,
            "b": 390.68800000000005,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            104
          ]
        }
      ],
      "orig": "1. Koehn, P.; Knowles, R. Six challenges for neural machine translation. arXiv 2017 , arXiv:1706.03872 .",
      "text": "1. Koehn, P.; Knowles, R. Six challenges for neural machine translation. arXiv 2017 , arXiv:1706.03872 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/145",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 392.372,
            "r": 560.49,
            "b": 424.368,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            272
          ]
        }
      ],
      "orig": "2. Daum' e III, H.; Jagarlamundi, J. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA, 19-24 June 2011; pp. 407-412.",
      "text": "2. Daum' e III, H.; Jagarlamundi, J. Domain adaptation for machine translation by mining unseen words. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Portland, OR, USA, 19-24 June 2011; pp. 407-412.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/146",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 426.052,
            "r": 560.49,
            "b": 447.944,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            158
          ]
        }
      ],
      "orig": "3. Saunders, D. Domain adaptation and multi-domain adaptation for neural machine translation: A survey. J. Artif. Intell. Res. 2022 , 75 , 351-424. [CrossRef]",
      "text": "3. Saunders, D. Domain adaptation and multi-domain adaptation for neural machine translation: A survey. J. Artif. Intell. Res. 2022 , 75 , 351-424. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/147",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 449.62800000000004,
            "r": 511.7,
            "b": 459.732,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            124
          ]
        }
      ],
      "orig": "4. Chu, C.; Wang, R. A survey of domain adaptation for machine translation. J. Inf. Process. 2020 , 28 , 413-426. [CrossRef]",
      "text": "4. Chu, C.; Wang, R. A survey of domain adaptation for machine translation. J. Inf. Process. 2020 , 28 , 413-426. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/148",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 461.41600000000005,
            "r": 530.74,
            "b": 471.52000000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            122
          ]
        }
      ],
      "orig": "5. Moslem, Y.; Haque, R.; Way, A. Adaptive machine translation with large language models. arXiv 2023 , arXiv:2301.13294 .",
      "text": "5. Moslem, Y.; Haque, R.; Way, A. Adaptive machine translation with large language models. arXiv 2023 , arXiv:2301.13294 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/149",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 473.20400000000006,
            "r": 560.49,
            "b": 503.51599999999996,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            266
          ]
        }
      ],
      "orig": "6. Popel, M.; Tomkova, M.; Tomek, J.; Kaiser, L.; Žušerkotz, J.; Bojar, O.; Žabkrotsky, T. Transforming machine translation: A deep learning system reaches news translation quality comparable to human professionals. Nat. Commun. 2020 , 11 , 4381. [CrossRef] [PubMed]",
      "text": "6. Popel, M.; Tomkova, M.; Tomek, J.; Kaiser, L.; Žušerkotz, J.; Bojar, O.; Žabkrotsky, T. Transforming machine translation: A deep learning system reaches news translation quality comparable to human professionals. Nat. Commun. 2020 , 11 , 4381. [CrossRef] [PubMed]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/150",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 505.2,
            "r": 560.49,
            "b": 537.196,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            357
          ]
        }
      ],
      "orig": "7. Moslem, Y.; Haque, R.; Kelleher, J.D.; Way, A. Domain-Specific Text Generation for Machine Translation. arXiv 2022 , arXiv:2208.05909 . Hatem, A.; Omar, N. Syntactic readinger for Arabic-English phrase-based machine translation. In Database Theory and Application , Bio-Science and Bio-Technology: Springer, Berlin/Heidelberg, Germany, 2010; pp. 198-206.",
      "text": "7. Moslem, Y.; Haque, R.; Kelleher, J.D.; Way, A. Domain-Specific Text Generation for Machine Translation. arXiv 2022 , arXiv:2208.05909 . Hatem, A.; Omar, N. Syntactic readinger for Arabic-English phrase-based machine translation. In Database Theory and Application , Bio-Science and Bio-Technology: Springer, Berlin/Heidelberg, Germany, 2010; pp. 198-206.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/151",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 538.88,
            "r": 560.49,
            "b": 559.0880000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            154
          ]
        }
      ],
      "orig": "8. Almahasese, Z.M. Assessment of Google and Microsoft Bing translation of journalistic texts. Int. J. Lang. Lit. Linguist. 2018 , 4 , 231-253. [CrossRef]",
      "text": "8. Almahasese, Z.M. Assessment of Google and Microsoft Bing translation of journalistic texts. Int. J. Lang. Lit. Linguist. 2018 , 4 , 231-253. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/152",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 562.456,
            "r": 560.49,
            "b": 582.664,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            242
          ]
        }
      ],
      "orig": "9. Sennrich, R.; Haddow, B.; Birch, A. Neural machine translation of rare words with subword units. arXiv 2015 , arXiv:1508.07909 . Costa-Jussa, M.R.; Fonollosa, J.A. Character-based neural machine translation. arXiv 2016 , arXiv:1603.00810 .",
      "text": "9. Sennrich, R.; Haddow, B.; Birch, A. Neural machine translation of rare words with subword units. arXiv 2015 , arXiv:1508.07909 . Costa-Jussa, M.R.; Fonollosa, J.A. Character-based neural machine translation. arXiv 2016 , arXiv:1603.00810 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/153",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 586.0319999999999,
            "r": 560.49,
            "b": 606.24,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            150
          ]
        }
      ],
      "orig": "10. Luong, M.T.; Manning, C.D. Achieving open vocabulary neural machine translation with hybrid word-character models. arXiv 2016 , arXiv:1604.00788 .",
      "text": "10. Luong, M.T.; Manning, C.D. Achieving open vocabulary neural machine translation with hybrid word-character models. arXiv 2016 , arXiv:1604.00788 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/154",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 609.608,
            "r": 503.37,
            "b": 619.712,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            119
          ]
        }
      ],
      "orig": "11. Muller, M.; Rios, A.; Sennrich, R. Domain robustness in neural machine translation. arXiv 2019 , arXiv:1911.03109 .",
      "text": "11. Muller, M.; Rios, A.; Sennrich, R. Domain robustness in neural machine translation. arXiv 2019 , arXiv:1911.03109 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/155",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 621.396,
            "r": 560.49,
            "b": 641.604,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            162
          ]
        }
      ],
      "orig": "12. Oudah, M.; Almahairi, A.; Habash, N. The impact of preprocessing on Arabic-English statistical and neural machine translation. arXiv 2019 , arXiv:1906.11751 .",
      "text": "12. Oudah, M.; Almahairi, A.; Habash, N. The impact of preprocessing on Arabic-English statistical and neural machine translation. arXiv 2019 , arXiv:1906.11751 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/156",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 644.972,
            "r": 560.49,
            "b": 678.652,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            296
          ]
        }
      ],
      "orig": "13. Papineni, K.; Roukos, S.; Ward, T.; Zhu, W.J. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Philadelphia, PA, USA, 6-12 July 2002; pp. 311-318.",
      "text": "13. Papineni, K.; Roukos, S.; Ward, T.; Zhu, W.J. BLEU: A method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. Association for Computational Linguistics, Philadelphia, PA, USA, 6-12 July 2002; pp. 311-318.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/157",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 680.336,
            "r": 560.49,
            "b": 700.544,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            262
          ]
        }
      ],
      "orig": "14. Bapna, A.; Arviazhagan, N.; Firat, O. Simple, scalable adaptation for neural machine translation. arXiv 2019 , arXiv:1909.08478 . Yang, S.; Wang, Y.; Chu, X. A survey of deep learning techniques for neural machine translation. arXiv 2020 , arXiv:2002.07526 .",
      "text": "14. Bapna, A.; Arviazhagan, N.; Firat, O. Simple, scalable adaptation for neural machine translation. arXiv 2019 , arXiv:1909.08478 . Yang, S.; Wang, Y.; Chu, X. A survey of deep learning techniques for neural machine translation. arXiv 2020 , arXiv:2002.07526 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/158",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 702.228,
            "r": 560.49,
            "b": 722.436,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            229
          ]
        }
      ],
      "orig": "15. Sutskever, I.; Vinyals, O.; Le, Q.V. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8-13 December 2014 .",
      "text": "15. Sutskever, I.; Vinyals, O.; Le, Q.V. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, Montreal, QC, Canada, 8-13 December 2014 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/159",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 725.804,
            "r": 560.49,
            "b": 759.484,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            283
          ]
        }
      ],
      "orig": "16. Johnson, M.; Schuster, M.; Le, Q.V.; Krikun, M.; Wu, Y.; Chen, Z.; Thorat, N.; Viˇegas, F.; Wattenberg, M.; Corrado, G.; et al. Google's multilingual neural machine translation system: Enabling zero-shot translation. Trans. Assoc. Comput. Linguist. 2017 , 5 , 339-351. [CrossRef]",
      "text": "16. Johnson, M.; Schuster, M.; Le, Q.V.; Krikun, M.; Wu, Y.; Chen, Z.; Thorat, N.; Viˇegas, F.; Wattenberg, M.; Corrado, G.; et al. Google's multilingual neural machine translation system: Enabling zero-shot translation. Trans. Assoc. Comput. Linguist. 2017 , 5 , 339-351. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/160",
      "parent": {
        "$ref": "#/groups/5"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 34.510000000000005,
            "t": 761.168,
            "r": 560.49,
            "b": 771.272,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            139
          ]
        }
      ],
      "orig": "20. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014 , arXiv:1409.0473 .",
      "text": "20. Bahdanau, D.; Cho, K.; Bengio, Y. Neural machine translation by jointly learning to align and translate. arXiv 2014 , arXiv:1409.0473 .",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/161",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 121.38,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/162",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "14 of 15",
      "text": "14 of 15"
    },
    {
      "self_ref": "#/texts/163",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 89.252,
            "r": 559.3,
            "b": 111.144,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            194
          ]
        }
      ],
      "orig": "21. Zhou, J.; Cao, Y.; Wang, X.; Li, P.; Xu, W. Deep recurrent models with fast-forward connections for neural machine translation. Trans. Assoc. Comput. Linguist. 2016 , 4 , 371-383. [CrossRef]",
      "text": "21. Zhou, J.; Cao, Y.; Wang, X.; Li, P.; Xu, W. Deep recurrent models with fast-forward connections for neural machine translation. Trans. Assoc. Comput. Linguist. 2016 , 4 , 371-383. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/164",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 112.828,
            "r": 559.3,
            "b": 134.72,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            140
          ]
        }
      ],
      "orig": "22. Ataman, D.; Aziz, W.; Birch, A. A latent morphology model for open-vocabulary neural machine translation. arXiv 2019 , arXiv:1910.13890.",
      "text": "22. Ataman, D.; Aziz, W.; Birch, A. A latent morphology model for open-vocabulary neural machine translation. arXiv 2019 , arXiv:1910.13890.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/165",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 136.404,
            "r": 559.3,
            "b": 158.296,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            173
          ]
        }
      ],
      "orig": "23. Meng, F.; Lu, Z.; Wang, M.; Li, H.; Jiang, W.; Liu, Q. Encoding source language with convolutional neural network for machine translation. arXiv 2015 , arXiv:1503.01838.",
      "text": "23. Meng, F.; Lu, Z.; Wang, M.; Li, H.; Jiang, W.; Liu, Q. Encoding source language with convolutional neural network for machine translation. arXiv 2015 , arXiv:1503.01838.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/166",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 159.98,
            "r": 560.49,
            "b": 181.87199999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            147
          ]
        }
      ],
      "orig": "24. Gehring, J.; Auli, M.; Grangier, D.; Dauphin, Y.N. A convolutional encoder model for neural machine translation. arXiv 2016 , arXiv:1611.02344.",
      "text": "24. Gehring, J.; Auli, M.; Grangier, D.; Dauphin, Y.N. A convolutional encoder model for neural machine translation. arXiv 2016 , arXiv:1611.02344.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/167",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 183.556,
            "r": 559.3,
            "b": 205.448,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            165
          ]
        }
      ],
      "orig": "25. Kalchbrenner, N.; Espeholt, L.; Simonyan, K.; Oord, A.v.d.; Graves, A.; Kavukcuoglu, K. Neural machine translation in linear time. arXiv 2016 , arXiv:1610.10099.",
      "text": "25. Kalchbrenner, N.; Espeholt, L.; Simonyan, K.; Oord, A.v.d.; Graves, A.; Kavukcuoglu, K. Neural machine translation in linear time. arXiv 2016 , arXiv:1610.10099.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/168",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 207.132,
            "r": 559.3,
            "b": 229.02400000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            237
          ]
        }
      ],
      "orig": "26. Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.: Dauphin, Y.N. Convolutional sequence to sequence learning. In Proceedings of the International Conference on Machine Learning, PMLR, Sydney, Australia, 6-11 August 2017; pp. 1243-1252.",
      "text": "26. Gehring, J.; Auli, M.; Grangier, D.; Yarats, D.: Dauphin, Y.N. Convolutional sequence to sequence learning. In Proceedings of the International Conference on Machine Learning, PMLR, Sydney, Australia, 6-11 August 2017; pp. 1243-1252.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/169",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 230.70800000000003,
            "r": 560.49,
            "b": 262.704,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            273
          ]
        }
      ],
      "orig": "27. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszekoreit, J.; Jones, L.; Gomez, A.N. Kaiser, L.; Polosukhin, I. Attention is you all in. Needs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, CA, USA, 4-9 December 2017.",
      "text": "27. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszekoreit, J.; Jones, L.; Gomez, A.N. Kaiser, L.; Polosukhin, I. Attention is you all in. Needs. In Proceedings of the 31st International Conference on Neural Information Processing Systems, Long Beach, CA, USA, 4-9 December 2017.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/170",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 264.388,
            "r": 559.3,
            "b": 284.596,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            246
          ]
        }
      ],
      "orig": "28. Luong, M.T.; Pham, H.; Manning, C.D. Effective approaches to attention-based neural machine translation. arXiv 2015 , arXiv:1508.04025. Stahlberg, F. Neural machine translation: A review. J. Artif. Intell. Res. 2020 , 69 , 343-448. [CrossRef]",
      "text": "28. Luong, M.T.; Pham, H.; Manning, C.D. Effective approaches to attention-based neural machine translation. arXiv 2015 , arXiv:1508.04025. Stahlberg, F. Neural machine translation: A review. J. Artif. Intell. Res. 2020 , 69 , 343-448. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/171",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 286.28000000000003,
            "r": 559.3,
            "b": 308.17199999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            165
          ]
        }
      ],
      "orig": "29. Devlin, J.; Chang, M.W.; Lee, K.; Tounantova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv 2018 , arXiv:1810.04805.",
      "text": "29. Devlin, J.; Chang, M.W.; Lee, K.; Tounantova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv 2018 , arXiv:1810.04805.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/172",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 309.856,
            "r": 559.3,
            "b": 319.96,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            135
          ]
        }
      ],
      "orig": "30. Antoun, W.; Baly, F.; Hajj, H. A.Rebater: Transformer-based model for Arabic language understanding. arXiv 2020 , arXiv:2003.00104.",
      "text": "30. Antoun, W.; Baly, F.; Hajj, H. A.Rebater: Transformer-based model for Arabic language understanding. arXiv 2020 , arXiv:2003.00104.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/173",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 321.644,
            "r": 559.3,
            "b": 343.536,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            232
          ]
        }
      ],
      "orig": "31. Lan, W.; Chen, Y.; Xu, W.; Ritter, A. Gigabert: Zero-shot transfer learning from english to arabic. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP) , Online, 16-20 November 2020.",
      "text": "31. Lan, W.; Chen, Y.; Xu, W.; Ritter, A. Gigabert: Zero-shot transfer learning from english to arabic. In Proceedings of the 2020 Conference on Empirical Methods on Natural Language Processing (EMNLP) , Online, 16-20 November 2020.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/174",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 345.21999999999997,
            "r": 559.3,
            "b": 377.216,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            286
          ]
        }
      ],
      "orig": "32. Abdul-Rafu, S.; Kiani, K.; Zafar, A.; Nawaz, R. Exploring transfer learning and domain data selection for the biomedical translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), Florence, Italy, 1-2 August 2019; pp. 156-163.",
      "text": "32. Abdul-Rafu, S.; Kiani, K.; Zafar, A.; Nawaz, R. Exploring transfer learning and domain data selection for the biomedical translation. In Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2), Florence, Italy, 1-2 August 2019; pp. 156-163.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/175",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 378.90000000000003,
            "r": 559.3,
            "b": 399.108,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            164
          ]
        }
      ],
      "orig": "33. Liu, B.; Huang, L. ParaMed: A parallel corpus for English-Chinese translation in the biomedical domain. BMC Med. Inform. Decis. Mak. 2021 , 21 , 258. [CrossRef]",
      "text": "33. Liu, B.; Huang, L. ParaMed: A parallel corpus for English-Chinese translation in the biomedical domain. BMC Med. Inform. Decis. Mak. 2021 , 21 , 258. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/176",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 400.792,
            "r": 559.3,
            "b": 432.788,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            319
          ]
        }
      ],
      "orig": "34. Martínez-Domínguez, R.; Rikters, M.; Vasiljevski, A.; Pinnis, M.; Reichenberg, P. Customized Neural Machine Translation Systems for the Swiss Legal Domain. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track), Online, 6-9 October 2020; pp. 217-223.",
      "text": "34. Martínez-Domínguez, R.; Rikters, M.; Vasiljevski, A.; Pinnis, M.; Reichenberg, P. Customized Neural Machine Translation Systems for the Swiss Legal Domain. In Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track), Online, 6-9 October 2020; pp. 217-223.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/177",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 434.47200000000004,
            "r": 559.3,
            "b": 456.36400000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            248
          ]
        }
      ],
      "orig": "35. Läubli, S.; Amrhein, C.; Düggelin, P.; Gonzalez, B.; Zwahenl, A.; Volk, M. Post-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domain. arXiv 2019 , arXiv:1906.01685.",
      "text": "35. Läubli, S.; Amrhein, C.; Düggelin, P.; Gonzalez, B.; Zwahenl, A.; Volk, M. Post-editing productivity with neural machine translation: An empirical assessment of speed and quality in the banking and finance domain. arXiv 2019 , arXiv:1906.01685.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/178",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 458.04800000000006,
            "r": 510.51,
            "b": 468.15200000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            119
          ]
        }
      ],
      "orig": "36. Grangier, D.; Iler, D. The trade-off of domain adaptation for neural language model. arXiv 2021 , arXiv:2109.10274.",
      "text": "36. Grangier, D.; Iler, D. The trade-off of domain adaptation for neural language model. arXiv 2021 , arXiv:2109.10274.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/179",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 469.83600000000007,
            "r": 559.3,
            "b": 490.044,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            160
          ]
        }
      ],
      "orig": "37. Poncelas, A.; Shterionov, D.; Way, A.; Wennerig, G.; Passban, P. Investigating Backtranslation in Neural Machine Translation. arXiv 2018 , arXiv:1804.06189.",
      "text": "37. Poncelas, A.; Shterionov, D.; Way, A.; Wennerig, G.; Passban, P. Investigating Backtranslation in Neural Machine Translation. arXiv 2018 , arXiv:1804.06189.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/180",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 491.72799999999995,
            "r": 559.3,
            "b": 523.724,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            289
          ]
        }
      ],
      "orig": "38. Chu, C.; Dabre, R.; Kurohashi, S. An empirical comparison of domain adaptation methods for neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Toronto, ON, Canada, 9-14 July 2017; pp. 385-391.",
      "text": "38. Chu, C.; Dabre, R.; Kurohashi, S. An empirical comparison of domain adaptation methods for neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Toronto, ON, Canada, 9-14 July 2017; pp. 385-391.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/181",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 525.408,
            "r": 559.3,
            "b": 547.3000000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            254
          ]
        }
      ],
      "orig": "39. Deng, Y.; Hu, Y.; Hu, D.; Lu, X. Wo, W. Factorized transformer for multi-domain neural machine translation. In Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020; pp. 4221-4230.",
      "text": "39. Deng, Y.; Hu, Y.; Hu, D.; Lu, X. Wo, W. Factorized transformer for multi-domain neural machine translation. In Proceedings of the Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020; pp. 4221-4230.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/182",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 548.984,
            "r": 559.3,
            "b": 582.664,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            283
          ]
        }
      ],
      "orig": "40. Bakr, H.A.; Shaalan, K.; Ziedan, A. I hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic. In Proceedings of the 6th International Conference on Informatics and Systems, infos2008, Cairo University, Citeseer, Cairo, Egypt, 27-28 March 2008.",
      "text": "40. Bakr, H.A.; Shaalan, K.; Ziedan, A. I hybrid approach for converting written Egyptian colloquial dialect into diacritized Arabic. In Proceedings of the 6th International Conference on Informatics and Systems, infos2008, Cairo University, Citeseer, Cairo, Egypt, 27-28 March 2008.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/183",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 584.348,
            "r": 559.3,
            "b": 616.3439999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            264
          ]
        }
      ],
      "orig": "41. Mohamed, E.; Mohit, B.; Oflaker, K. Transforming standard Arabic to colloquial Arabic. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Jju Island, Republic Korea, 8-14 July 2012; pp. 176-180.",
      "text": "41. Mohamed, E.; Mohit, B.; Oflaker, K. Transforming standard Arabic to colloquial Arabic. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Jju Island, Republic Korea, 8-14 July 2012; pp. 176-180.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/184",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 618.028,
            "r": 559.3,
            "b": 639.92,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            233
          ]
        }
      ],
      "orig": "42. Habash, N.; Hu, J. Improving Arabic-Chinese statistical machine translation using English as pivot language. In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece, 30-31 March 2009; pp. 173-181.",
      "text": "42. Habash, N.; Hu, J. Improving Arabic-Chinese statistical machine translation using English as pivot language. In Proceedings of the Fourth Workshop on Statistical Machine Translation, Athens, Greece, 30-31 March 2009; pp. 173-181.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/185",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 641.604,
            "r": 559.3,
            "b": 663.496,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            147
          ]
        }
      ],
      "orig": "44. Nagoudi, E.M.B.; Elmadany, A.; Abdul-Mageed, M. AraT5: Text-to-text transformers for Arabic language generation. arXiv 2021 , arXiv:2109.12068.",
      "text": "44. Nagoudi, E.M.B.; Elmadany, A.; Abdul-Mageed, M. AraT5: Text-to-text transformers for Arabic language generation. arXiv 2021 , arXiv:2109.12068.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/186",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 665.1800000000001,
            "r": 559.3,
            "b": 687.072,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            183
          ]
        }
      ],
      "orig": "45. Zakraoui, J.; Saleh, M.; Al-Maadeed, S.; Alja'am, J.M. Arabic Machine Translation: A Survey With Challenges and Future Directions. IEEE Access 2021 , 9 , 161445-161468. [CrossRef]",
      "text": "45. Zakraoui, J.; Saleh, M.; Al-Maadeed, S.; Alja'am, J.M. Arabic Machine Translation: A Survey With Challenges and Future Directions. IEEE Access 2021 , 9 , 161445-161468. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/187",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 688.756,
            "r": 559.3,
            "b": 710.648,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            168
          ]
        }
      ],
      "orig": "46. Ameur, M.S.H.; Meziane, F.; Guessouum, A. Arabic machine translation: A survey of the latest trends and challenges. Comput. Sci. Rev. 2020 , 38 , 100305. [CrossRef]",
      "text": "46. Ameur, M.S.H.; Meziane, F.; Guessouum, A. Arabic machine translation: A survey of the latest trends and challenges. Comput. Sci. Rev. 2020 , 38 , 100305. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/188",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 712.332,
            "r": 503.37,
            "b": 722.436,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            119
          ]
        }
      ],
      "orig": "47. Habash, N.Y. Introduction to Arabic natural language processing. Synth. Lect. Hum. Lang. Technol. 2010 , 3 , 1-187.",
      "text": "47. Habash, N.Y. Introduction to Arabic natural language processing. Synth. Lect. Hum. Lang. Technol. 2010 , 3 , 1-187.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/189",
      "parent": {
        "$ref": "#/groups/6"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 14,
          "bbox": {
            "l": 34.510000000000005,
            "t": 724.12,
            "r": 559.3,
            "b": 746.0120000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            184
          ]
        }
      ],
      "orig": "48. Ehab, R.; Amer, E.; Gadallah, M. English-Arabic hybrid machine translation system using EBMT and translation memory. Int. J. Adv. Comput. Sci. Appl. 2019 , 10 , 195-203. [CrossRef]",
      "text": "48. Ehab, R.; Amer, E.; Gadallah, M. English-Arabic hybrid machine translation system using EBMT and translation memory. Int. J. Adv. Comput. Sci. Appl. 2019 , 10 , 195-203. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/190",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 52.204,
            "r": 132.09,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            27
          ]
        }
      ],
      "orig": "Appl. Sci. 2024 , 14 , 7088",
      "text": "Appl. Sci. 2024 , 14 , 7088"
    },
    {
      "self_ref": "#/texts/191",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "furniture",
      "label": "page_header",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 531.9300000000001,
            "t": 52.204,
            "r": 559.3,
            "b": 62.308,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            8
          ]
        }
      ],
      "orig": "15 of 15",
      "text": "15 of 15"
    },
    {
      "self_ref": "#/texts/192",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 90.93599999999999,
            "r": 560.49,
            "b": 138.088,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            381
          ]
        }
      ],
      "orig": "49. Sajad, H.; Abdelali, A.; Durrani, N.; Dalvi, F. AraBench: Benchmarking Dialectal Arabic-English Machine Translation. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, 8-13 December 2020; Scott, D., Bel, N., Zong, C., Eds.; International Committee on Computational Linguistics: New York, NY, USA, 2020; pp. 5094-5107. [CrossRef]",
      "text": "49. Sajad, H.; Abdelali, A.; Durrani, N.; Dalvi, F. AraBench: Benchmarking Dialectal Arabic-English Machine Translation. In Proceedings of the 28th International Conference on Computational Linguistics, Barcelona, Spain, 8-13 December 2020; Scott, D., Bel, N., Zong, C., Eds.; International Committee on Computational Linguistics: New York, NY, USA, 2020; pp. 5094-5107. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/193",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 139.77200000000002,
            "r": 559.3,
            "b": 159.98,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            163
          ]
        }
      ],
      "orig": "50. Khondaker, M.T.I.; Waehed, A.; Nagoudi, E.M.B.; Abdul-Mageed, M. GPTraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. arXiv 2023 , arXiv:2305.14976.",
      "text": "50. Khondaker, M.T.I.; Waehed, A.; Nagoudi, E.M.B.; Abdul-Mageed, M. GPTraEval: A Comprehensive Evaluation of ChatGPT on Arabic NLP. arXiv 2023 , arXiv:2305.14976.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/194",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 161.66400000000002,
            "r": 560.49,
            "b": 181.87199999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            215
          ]
        }
      ],
      "orig": "51. Antoun, W.; Baly, F.; Hajj, H. AraGPT2: Pre-Trained Transformer for Arabic Language Generation. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, Kyiv, Ukraine, 19 April 2021; pp. 196-207.",
      "text": "51. Antoun, W.; Baly, F.; Hajj, H. AraGPT2: Pre-Trained Transformer for Arabic Language Generation. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, Kyiv, Ukraine, 19 April 2021; pp. 196-207.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/195",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 183.556,
            "r": 560.49,
            "b": 203.76399999999998,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            150
          ]
        }
      ],
      "orig": "52. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language Models are Unsupervised Multitask Learners. Openai Blog 2019 , 1 , 9.",
      "text": "52. Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever, I. Language Models are Unsupervised Multitask Learners. Openai Blog 2019 , 1 , 9.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/196",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 205.448,
            "r": 560.49,
            "b": 264.388,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            565
          ]
        }
      ],
      "orig": "53. Xue, L.; Constant N.; Roberts, A.; Kale, M.; Al-Frou, R.; Siddhant, A.; Barua, A.; Rafelf, C. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online, 7 June 2021; Toutanova, K., Rumshisky, A., Letzemoyer, L., Hakkan-Turi, D., Beltagy, L., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y., Eds.; Association for Computational Linguistics: Stroudsburg, PA, USA, 2021; pp. 483-498. [CrossRef]",
      "text": "53. Xue, L.; Constant N.; Roberts, A.; Kale, M.; Al-Frou, R.; Siddhant, A.; Barua, A.; Rafelf, C. mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Online, 7 June 2021; Toutanova, K., Rumshisky, A., Letzemoyer, L., Hakkan-Turi, D., Beltagy, L., Bethard, S., Cotterell, R., Chakraborty, T., Zhou, Y., Eds.; Association for Computational Linguistics: Stroudsburg, PA, USA, 2021; pp. 483-498. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/197",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 266.072,
            "r": 560.49,
            "b": 284.596,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            217
          ]
        }
      ],
      "orig": "54. Tiedemann, J.; Tothtingal, S. OUP-MT-Building open translation services for the World. In Proceedings of the European Association for Machine Translation Conferences/Workshops, Lisbon, Portugal, 3-5 November 2020.",
      "text": "54. Tiedemann, J.; Tothtingal, S. OUP-MT-Building open translation services for the World. In Proceedings of the European Association for Machine Translation Conferences/Workshops, Lisbon, Portugal, 3-5 November 2020.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/198",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 286.28000000000003,
            "r": 560.49,
            "b": 306.488,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            224
          ]
        }
      ],
      "orig": "55. Tiedemann, J. The Tatoeba Translation Challenge-Realistic Data Sets for Low Resource and Multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, Lisbon, Portugal, 3-5 November 2020; pp. 1174-1182.",
      "text": "55. Tiedemann, J. The Tatoeba Translation Challenge-Realistic Data Sets for Low Resource and Multilingual MT. In Proceedings of the Fifth Conference on Machine Translation, Lisbon, Portugal, 3-5 November 2020; pp. 1174-1182.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/199",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 308.17199999999997,
            "r": 560.49,
            "b": 340.168,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            323
          ]
        }
      ],
      "orig": "56. Kudo, T.; Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, 31 October 2014; pp. 66-71. [CrossRef]",
      "text": "56. Kudo, T.; Richardson, J. SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018: System Demonstrations, Brussels, Belgium, 31 October 2014; pp. 66-71. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/200",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 341.85200000000003,
            "r": 560.49,
            "b": 362.06,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            232
          ]
        }
      ],
      "orig": "57. Team, N.; Costa-jussa, M.R.; Cross, J.; Celé, O.; Elbay, M.; Heelfd, K.; Heffernan, K.; Kalbassi, E.; Lam, J.; Licht, D.; et al. No Language Left Behind: Scaling Human-Centered Machine Translation. arXiv 2022 , arXiv:2207.04672.",
      "text": "57. Team, N.; Costa-jussa, M.R.; Cross, J.; Celé, O.; Elbay, M.; Heelfd, K.; Heffernan, K.; Kalbassi, E.; Lam, J.; Licht, D.; et al. No Language Left Behind: Scaling Human-Centered Machine Translation. arXiv 2022 , arXiv:2207.04672.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/201",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 363.74399999999997,
            "r": 559.3,
            "b": 383.952,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            210
          ]
        }
      ],
      "orig": "58. Popović, M. chrF: Character n-gram-Fscore for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, Lisbon, Portugal, 7-18 September 2015; pp. 392-395. [CrossRef]",
      "text": "58. Popović, M. chrF: Character n-gram-Fscore for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, Lisbon, Portugal, 7-18 September 2015; pp. 392-395. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/202",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 385.636,
            "r": 560.49,
            "b": 421.0,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            297
          ]
        }
      ],
      "orig": "59. Snover, M.; Dorr, B.; Schwartz, R.; Micullica, L.; Mahkoul, J. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, Cambridge, MA, USA, 5-8 October 2006; pp. 223-231.",
      "text": "59. Snover, M.; Dorr, B.; Schwartz, R.; Micullica, L.; Mahkoul, J. A study of translation edit rate with targeted human annotation. In Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers, Cambridge, MA, USA, 5-8 October 2006; pp. 223-231.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/203",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 422.684,
            "r": 560.49,
            "b": 456.36400000000003,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            249
          ]
        }
      ],
      "orig": "60. Rei, R.; Stewart, C.; Farinha, A.C.; Lavie, A. COMET: A Neural Framework for MT Evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, 16-20 November 2020; pp. 2685-2702. [CrossRef]",
      "text": "60. Rei, R.; Stewart, C.; Farinha, A.C.; Lavie, A. COMET: A Neural Framework for MT Evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Online, 16-20 November 2020; pp. 2685-2702. [CrossRef]",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/204",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 458.04800000000006,
            "r": 560.49,
            "b": 478.256,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            143
          ]
        }
      ],
      "orig": "61. Zhang, T.; Kishore, V.; Wu, F.; Wiehberger, K.Q.; Artzi, Y. BERTScore: Evaluating Text Generation with BERT. arXiv 2020 , arXiv:1904.09675.",
      "text": "61. Zhang, T.; Kishore, V.; Wu, F.; Wiehberger, K.Q.; Artzi, Y. BERTScore: Evaluating Text Generation with BERT. arXiv 2020 , arXiv:1904.09675.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/205",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 479.93999999999994,
            "r": 560.49,
            "b": 500.14799999999997,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            175
          ]
        }
      ],
      "orig": "62. Peng, K.; Ding, L.; Zhong, Q.; Shen, L.; Liu, X.; Zhang, M.; Ouyang, Y.; Tao, D. Towards Making the Most of ChatGPT for Machine Translation. arXiv 2023 , arXiv:2303.13780.",
      "text": "62. Peng, K.; Ding, L.; Zhong, Q.; Shen, L.; Liu, X.; Zhang, M.; Ouyang, Y.; Tao, D. Towards Making the Most of ChatGPT for Machine Translation. arXiv 2023 , arXiv:2303.13780.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/206",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 501.832,
            "r": 560.49,
            "b": 522.04,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            188
          ]
        }
      ],
      "orig": "63. Khoshafah, F. ChatGPT for Arabic-English Translation: Evaluating the Accuracy. 2023. Available online: https://www. researchsquares.com/article/rs-2814154/2 (accessed on 12 July 2024).",
      "text": "63. Khoshafah, F. ChatGPT for Arabic-English Translation: Evaluating the Accuracy. 2023. Available online: https://www. researchsquares.com/article/rs-2814154/2 (accessed on 12 July 2024).",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/207",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 523.724,
            "r": 560.49,
            "b": 543.932,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            183
          ]
        }
      ],
      "orig": "64. Alyafei, A.; Zalisi, M.M.; Alkishamissi, B.; Luan, H.; Balqian, H.; Alarei, E.; Fadel, A. Tagxim: Evaluating ARLNpic NLP Tasks Using ChatGPT Models. arXiv 2023 , arXiv:2306.16322.",
      "text": "64. Alyafei, A.; Zalisi, M.M.; Alkishamissi, B.; Luan, H.; Balqian, H.; Alarei, E.; Fadel, A. Tagxim: Evaluating ARLNpic NLP Tasks Using ChatGPT Models. arXiv 2023 , arXiv:2306.16322.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/208",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 545.616,
            "r": 560.49,
            "b": 579.2959999999999,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            278
          ]
        }
      ],
      "orig": "65. Banimelhem, O.; Amayreh, W. Is ChatGPT a Good English to Arabic Machine Translation Tool? In Proceedings of the 2023 14th International Conference on Information and Communication Systems (ICICS), Irbid, Jordan, 21-23 November 2023; IEEE: Piscataway, NJ, USA, 2023; pp: 1-6.",
      "text": "65. Banimelhem, O.; Amayreh, W. Is ChatGPT a Good English to Arabic Machine Translation Tool? In Proceedings of the 2023 14th International Conference on Information and Communication Systems (ICICS), Irbid, Jordan, 21-23 November 2023; IEEE: Piscataway, NJ, USA, 2023; pp: 1-6.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/209",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 580.9799999999999,
            "r": 560.49,
            "b": 606.24,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            224
          ]
        }
      ],
      "orig": "66. Hendy, A.; Abdelrehim, M.; Sharaf, A.; Raunak, V.; Gabr, M.; Matsushita, H.; Kim, Y.J.; Afify, M.; Awadalla, H.H. How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation. arXiv 2023 , arXiv:2302.09210.",
      "text": "66. Hendy, A.; Abdelrehim, M.; Sharaf, A.; Raunak, V.; Gabr, M.; Matsushita, H.; Kim, Y.J.; Afify, M.; Awadalla, H.H. How Good Are GPT Models at Machine Translation? A Comprehensive Evaluation. arXiv 2023 , arXiv:2302.09210.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/210",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 607.924,
            "r": 560.49,
            "b": 628.132,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            183
          ]
        }
      ],
      "orig": "67. Coughlin, D. Correlation and human assessments of machine translation quality. In Proceedings of the Machine Translation Summit II: Papers, New Orleans, USA, 23-25 September 2003.",
      "text": "67. Coughlin, D. Correlation and human assessments of machine translation quality. In Proceedings of the Machine Translation Summit II: Papers, New Orleans, USA, 23-25 September 2003.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/211",
      "parent": {
        "$ref": "#/groups/7"
      },
      "children": [],
      "content_layer": "body",
      "label": "list_item",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 629.816,
            "r": 560.49,
            "b": 675.284,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            432
          ]
        }
      ],
      "orig": "68. Rei, R.; Stewart, C.; Farinha, A.C.; Lavie, A. Unbabel's Participation in the WMT20 Metrics Shared Task. In Proceedings of the Fifth Conference on Machine Translation, Online, 30 May-12 June 2020; Barrault, L., Bojar, O., Bougares, F., Chatterjee, R., Costa-jussa, M.R., Federmann, C., Fishel, M., Fraser, A., Graham, Y., Guzman, P., et al., Eds.; Association for Computational Linguistics: Portland, OR, USA, 2020; pp. 911-920.",
      "text": "68. Rei, R.; Stewart, C.; Farinha, A.C.; Lavie, A. Unbabel's Participation in the WMT20 Metrics Shared Task. In Proceedings of the Fifth Conference on Machine Translation, Online, 30 May-12 June 2020; Barrault, L., Bojar, O., Bougares, F., Chatterjee, R., Costa-jussa, M.R., Federmann, C., Fishel, M., Fraser, A., Graham, Y., Guzman, P., et al., Eds.; Association for Computational Linguistics: Portland, OR, USA, 2020; pp. 911-920.",
      "enumerated": false,
      "marker": ""
    },
    {
      "self_ref": "#/texts/212",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "text",
      "prov": [
        {
          "page_no": 15,
          "bbox": {
            "l": 34.510000000000005,
            "t": 688.756,
            "r": 560.49,
            "b": 722.436,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            369
          ]
        }
      ],
      "orig": "Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.",
      "text": "Disclaimer/Publisher's Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content."
    }
  ],
  "pictures": [
    {
      "self_ref": "#/pictures/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [
        {
          "page_no": 5,
          "bbox": {
            "l": 172.54999999999998,
            "t": 682.0200000000001,
            "r": 523.6,
            "b": 751.064,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/63"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": [],
      "ai_analysis": {
        "image_type": "CONCEPTUAL",
        "description": "The image is a flowchart illustrating an Arabic text summarization process. It's labeled \"(b) Arabic Text Summarization\" at the bottom right. The flow begins with \"Prompt: Arabic Title,\" represented by a rectangular box containing stylized vertical lines, symbolizing text. An arrow points to a box labeled \"mGPT2,\" indicating this is where the initial processing occurs. Another arrow leads from mGPT2 to a larger rectangular box representing \"Symbolic long-form Arabic Text,\" depicted as a grid of smaller squares. This box then connects via an arrow to another box labeled \"mT5.\" Finally, an arrow points from mT5 to a final output box titled \"Arabic Summary,\" which mirrors the initial input format with stylized vertical lines. The overall flow demonstrates a sequential process starting with an Arabic title prompt, being processed by mGPT2 and mT5 models, ultimately resulting in an Arabic summary. (Original image data removed for NLP processing)",
        "analysis_timestamp": "2025-08-19 01:01:53",
        "model_used": "google/gemma-3-12b-it-gguf",
        "will_replace_image": true,
        "search_keywords": [
          "Arabic NLP",
          "Text Summarization",
          "mGPT2"
        ],
        "web_context": {
          "search_query": "Arabic NLP Text Summarization",
          "search_keywords": [
            "Arabic NLP",
            "Text Summarization",
            "mGPT2"
          ],
          "sources_count": 5,
          "sources": [
            {
              "title": "SalmanAlshawmar/Arabic-NLP-Project - GitHub",
              "body": "A comprehensive Natural Language Processing project exploring Arabic text classification and summarization using both traditional machine learning and modern transformer-based approaches.",
              "url": "https://github.com/SalmanAlshawmar/Arabic-NLP-Project"
            },
            {
              "title": "Exploring Advanced Arabic Text Summarization with Python ...",
              "body": "Jul 24, 2024 · In this blog post, we will explore the sophisticated ecosystem of these models, from GPT-2 and BERT to the latest LLMs, and their role in advancing Arabic text summarization .",
              "url": "https://medium.com/@alroumi.abdulmajeed/exploring-advanced-arabic-text-summarization-with-python-transformers-and-large-language-models-c9c637046827"
            },
            {
              "title": "Enhanced model for abstractive Arabic text summarization ...",
              "body": "Jan 30, 2025 · We introduce an innovative framework using Arabic Named Entity Recognition to enhance abstractive summarization , crucial for NLP applications like question answering and knowledge graph construction. Our model, based on natural language generation techniques, adapts to diverse datasets.",
              "url": "https://link.springer.com/article/10.1007/s00521-024-10949-x"
            },
            {
              "title": "Arabic text summarization using deep learning approach A Review of Arabic Text Summarization: Methods, Datasets, and ... Arabic text summarization using deep learning approach Arabic text summarization using deep learning approach Arabic text summarization using deep learning approach Arabic text summarization using deep learning approach Arabic text summarization using deep learning approach Arabic text summarization using deep learning approach Arabic Text Summarization using transformer-based architectures",
              "body": "The sequence-to-sequence framework consists of two parts: a neural network for the encoder and another network for the decoder. The source text, reference summary data is tokenized and fed to the encoder and decoder networks respectively during training. The encoder network reads the source text and transforms it into a potentially useful vector re... See full list on journalofbigdata.springeropen.com A mapping of the decoder state at each time step with all the encoder states into an attention vector, helps produce a context vector which is a weighted sum of the encoder states. Incorporating this context vector at each decoding time step helps improve text generation . See full list on journalofbigdata.springeropen.com Greedy decoding When using greedy decoding, the model at any time step has only one single hypothesis. Since a text sequence can be the most probable despite including tokens that are not the most probable at each time step, greedy decoding is seldom used in practice. Beam decoding When using beam search decoding the model iteratively expands each hypothesis one token at a time and in the end of each iteration, it only keeps the beam-size best ones as shown in Fig. 3. Small beam sizes are able to yield good results in terms of ROUGE score while larger beam sizes can yield worse results. To make decoding efficient the decoder expands only hypotheses that look promising. Bad hypotheses should be pruned early to avoid wasting time on them, but pruning compromises optimality. See full list on journalofbigdata.springeropen.com Additionally, we propose six standardized evaluation splits tailored to distinct summarization goals, promoting reproducibility and fair comparison. To address inconsistencies, we also recommend... Does the sequence-to-sequence framework apply to Arabic text summarization? In this research, we are re-implementing the basic summarization model that applies the sequence-to-sequence framework on the Arabic language, which has not witnessed the employment of this model in the text summarization before . Initially, we build an Arabic data set of summarized article headlines. What are the two methods used to summarize a text? There are two basic methodologies used to summarize the texts, which are extractive summarization —from which most systems with good results came out—and abstractive summarization that simulate human summarization . Why is the Arabic language better than other datasets? We assume that this improvement is due to two main reasons: 1. The nature of the dataset, which consists of short reference summaries (headlines) comparing to other datasets. 2. The nature of the Arabic language in terms of more language grammar consistency within written text , which contributed to this improvement. Which abstractive model is better for the Arabic dataset? By comparing the results of the models that we applied to the Arabic dataset, AHS, with other abstractive models applied to Gigaword and CNN Daily Mail datasets, we note that the pointer-generator model with length penalty, has achieved better results for the Arabic dataset as shown in Table 5 and illustrated is Fig. 8. How do deep neural networks improve text summarization? Text summarization, along other tasks like text translation and sentiment analysis, used deep neural network models to enhance results. The new methods of text summarization are subject to a sequence-to-sequence framework of encoder–decoder model , which is composed of neural networks trained jointly on both input and output. What is the task of text summarization? The task of text summarization is one of the most important challenges that faces computer capabilities with all its new advances. This task is based on generating short text from longer text so that the short text contains the most important info of the original text . Text summarizing is one of the most challenging tasks in natural language processing ( NLP ). This task is addressed in a large number of research projects and papers in the literature, but most of them focused on English language. Few studies are dealing with the complex Arabic language.",
              "url": "https://journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00386-7"
            },
            {
              "title": "A Review of Arabic Text Summarization: Methods, Datasets, and ...",
              "body": "Additionally, we propose six standardized evaluation splits tailored to distinct summarization goals, promoting reproducibility and fair comparison. To address inconsistencies, we also recommend...",
              "url": "https://www.researchgate.net/publication/393308182_A_Review_of_Arabic_Text_Summarization_Methods_Datasets_and_Evaluation_Metrics_with_a_Proposed_Solution/fulltext/68654f94e9b6c13c89e61641/A-Review-of-Arabic-Text-Summarization-Methods-Datasets-and-Evaluation-Metrics-with-a-Proposed-Solution.pdf"
            }
          ],
          "ai_summary": "Arabic text summarization is a growing area within Natural Language Processing (NLP), focusing on automatically generating concise summaries from Arabic text. Approaches range from traditional machine learning to modern transformer-based models like BERT and GPT-2, with recent advancements exploring Large Language Models (LLMs). Two primary methods exist: extractive summarization, which selects existing sentences, and abstractive summarization, which generates new phrases – the latter aiming to mimic human summarization. A common technique utilizes a sequence-to-sequence framework involving encoder and decoder neural networks, often enhanced with attention mechanisms for improved text generation. Researchers are also focusing on incorporating Arabic Named Entity Recognition and developing standardized evaluation datasets to improve model performance and reproducibility within this relatively understudied field compared to English NLP.",
          "search_timestamp": "2025-08-19 01:01:57"
        },
        "enriched_description": "The image is a flowchart illustrating an Arabic text summarization process. It's labeled \"(b) Arabic Text Summarization\" at the bottom right. The flow begins with \"Prompt: Arabic Title,\" represented by a rectangular box containing stylized vertical lines, symbolizing text. An arrow points to a box labeled \"mGPT2,\" indicating this is where the initial processing occurs. Another arrow leads from mGPT2 to a larger rectangular box representing \"Symbolic long-form Arabic Text,\" depicted as a grid of smaller squares. This box then connects via an arrow to another box labeled \"mT5.\" Finally, an arrow points from mT5 to a final output box titled \"Arabic Summary,\" which mirrors the initial input format with stylized vertical lines. The overall flow demonstrates a sequential process starting with an Arabic title prompt, being processed by mGPT2 and mT5 models, ultimately resulting in an Arabic summary. Additional context: Arabic text summarization is a growing area within Natural Language Processing (NLP), focusing on automatically generating concise summaries from Arabic text. Approaches range from traditional machine learning to modern transformer-based models like BERT and GPT-2, with recent advancements exploring Large Language Models (LLMs). Two primary methods exist: extractive summarization, which selects existing sentences, and abstractive summarization, which generates new phrases – the latter aiming to mimic human summarization. A common technique utilizes a sequence-to-sequence framework involving encoder and decoder neural networks, often enhanced with attention mechanisms for improved text generation. Researchers are also focusing on incorporating Arabic Named Entity Recognition and developing standardized evaluation datasets to improve model performance and reproducibility within this relatively understudied field compared to English NLP.",
        "images_removed_in_step2": true
      }
    },
    {
      "self_ref": "#/pictures/4",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 177.31,
            "t": 245.86399999999998,
            "r": 447.44,
            "b": 326.696,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/68"
        }
      ],
      "references": [],
      "footnotes": [],
      "annotations": [],
      "ai_analysis": {
        "image_type": "CONCEPTUAL",
        "description": "The image is a flowchart illustrating a translation process. On the left side of the diagram, there's an icon representing \"English Summary and Synthetic long-form English text.\" This icon depicts a document with multiple pages, indicated by horizontal lines across its surface. An arrow points to the right towards a triangular box labeled \"OPUS-EN-AR\". This box represents a translation system or process that converts from English to Arabic. Another arrow then extends from this box to an icon on the right side of the diagram, representing \"Arabic Summary and Synthetic long-form Arabic text.\" This icon mirrors the appearance of the initial English document icon, also depicting a multi-page document with horizontal lines. The overall flow demonstrates the transformation of English text into its Arabic equivalent using the OPUS-EN-AR system. (Original image data removed for NLP processing)",
        "analysis_timestamp": "2025-08-19 01:02:02",
        "model_used": "google/gemma-3-12b-it-gguf",
        "will_replace_image": true,
        "search_keywords": [
          "machine translation",
          "OPUS project",
          "natural language processing"
        ],
        "web_context": {
          "search_query": "machine translation OPUS project",
          "search_keywords": [
            "machine translation",
            "OPUS project",
            "natural language processing"
          ],
          "sources_count": 5,
          "sources": [
            {
              "title": "GitHub GitHub - Helsinki-NLP/Opus-MT: Open neural machine translation models and web services",
              "body": "Open neural machine translation models and web services - Helsinki-NLP/ Opus -MT",
              "url": "https://github.com/Helsinki-NLP/Opus-MT"
            },
            {
              "title": "GitHub GitHub - Helsinki-NLP/OPUS-MT-app: Fast and secure translation on your local machine, powered by marian and Bergamot.",
              "body": "Fast and secure translation on your local machine with a GUI, powered by marian and Bergamot. The app is an adaptation of translateLocally and integrates publically avaiable translation models from the OPUS -MT project to bring fast and secure machine translation to the desktop of end users.",
              "url": "https://github.com/Helsinki-NLP/OPUS-MT-app"
            },
            {
              "title": "arXiv [2212.01936] Democratizing Neural Machine Translation with OPUS-MT",
              "body": "July 4, 2023 - This paper presents the OPUS ecosystem with a focus on the development of open machine translation models and tools, and their integration into end-user applications, development platforms and professional workflows. We discuss our on-going mission of increasing language coverage and translation ...",
              "url": "https://arxiv.org/abs/2212.01936"
            },
            {
              "title": "ACL Anthology OPUS-MT – Building open translation services for the World - ACL Anthology",
              "body": "This paper presents OPUS-MT a project that focuses on the development of free resources and tools for machine translation . The current status is a repository of over 1,000 pre-trained neural machine translation models that are ready to be launched in on-line translation services.",
              "url": "https://aclanthology.org/2020.eamt-1.61/"
            },
            {
              "title": "RWS OPUS project: Opus magnum of free Machine Translation - RWS",
              "body": "March 24, 2022 - The OPUS project was created to make every language, however big or small, accessible to everyone across the globe. Read more about it here at RWS today.",
              "url": "https://www.rws.com/blog/the-opus-project/"
            }
          ],
          "ai_summary": "The OPUS project is an initiative focused on democratizing machine translation by providing free and open resources for language accessibility worldwide. At its core, OPUS-MT specifically develops pre-trained neural machine translation models – currently exceeding 1,000 – which are readily available for use in online services and applications. These models are often integrated into user-friendly tools like the \"OPUS-MT-app\" that allows for local, secure translation. The project aims to expand language coverage significantly, supporting both major and less common languages. Ultimately, OPUS strives to make machine translation accessible to everyone, fostering communication and understanding across diverse linguistic communities through open development platforms and workflows. It's essentially a large repository of readily usable machine translation models and tools.",
          "search_timestamp": "2025-08-19 01:02:06"
        },
        "enriched_description": "The image is a flowchart illustrating a translation process. On the left side of the diagram, there's an icon representing \"English Summary and Synthetic long-form English text.\" This icon depicts a document with multiple pages, indicated by horizontal lines across its surface. An arrow points to the right towards a triangular box labeled \"OPUS-EN-AR\". This box represents a translation system or process that converts from English to Arabic. Another arrow then extends from this box to an icon on the right side of the diagram, representing \"Arabic Summary and Synthetic long-form Arabic text.\" This icon mirrors the appearance of the initial English document icon, also depicting a multi-page document with horizontal lines. The overall flow demonstrates the transformation of English text into its Arabic equivalent using the OPUS-EN-AR system. Additional context: The OPUS project is an initiative focused on democratizing machine translation by providing free and open resources for language accessibility worldwide. At its core, OPUS-MT specifically develops pre-trained neural machine translation models – currently exceeding 1,000 – which are readily available for use in online services and applications. These models are often integrated into user-friendly tools like the \"OPUS-MT-app\" that allows for local, secure translation. The project aims to expand language coverage significantly, supporting both major and less common languages. Ultimately, OPUS strives to make machine translation accessible to everyone, fostering communication and understanding across diverse linguistic communities through open development platforms and workflows. It's essentially a large repository of readily usable machine translation models and tools.",
        "images_removed_in_step2": true
      }
    },
    {
      "self_ref": "#/pictures/5",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 166.60000000000002,
            "t": 223.972,
            "r": 486.71,
            "b": 449.62800000000004,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [],
      "references": [],
      "footnotes": [],
      "annotations": [
        {
          "kind": "classification",
          "provenance": "load_from_doctags",
          "predicted_classes": [
            {
              "class_name": "line_chart",
              "confidence": 1.0
            }
          ]
        },
        {
          "kind": "tabular_chart_data",
          "title": "line_chart",
          "chart_data": {
            "table_cells": [
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "metric",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "chart",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "model",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "ChatGPT-3.5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              }
            ],
            "num_rows": 14,
            "num_cols": 4,
            "grid": [
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "metric",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "chart",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "model",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "ChatGPT-3.5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ]
            ]
          }
        }
      ],
      "ai_analysis": {
        "image_type": "DATA_VISUALIZATION",
        "description": "The image is a line graph comparing the performance of three language models – BLOUS (bt-big), NLLB 600M, and ChatGPT-3.5 – across five metrics. The x-axis represents the model name, while the y-axis indicates the score ranging from 0 to 90. A legend on the right side identifies each colored line with its corresponding metric: spe (purple), chr (red), TER (green), con (blue), and BERT (orange). (Original image data removed for NLP processing)",
        "analysis_timestamp": "2025-08-19 01:02:12",
        "model_used": "google/gemma-3-12b-it-gguf",
        "will_replace_image": true,
        "chart_data_extraction": {
          "extraction_success": true,
          "data_points_count": 15,
          "chart_type": "line_chart",
          "x_axis_label": "model",
          "y_axis_labels": [
            "spE",
            "chr",
            "TER",
            "COI",
            "BEE"
          ],
          "data_ranges": {
            "x_range": null,
            "y_ranges": {}
          },
          "extraction_method": "image+deplot_verification",
          "extraction_timestamp": "2025-08-19 01:03:09",
          "datasets": {
            "spE": [
              [
                0,
                14.4
              ],
              [
                1,
                14.2
              ],
              [
                2,
                25.7
              ]
            ],
            "chr": [
              [
                0,
                13.9
              ],
              [
                1,
                13.7
              ],
              [
                2,
                25.8
              ]
            ],
            "TER": [
              [
                0,
                4.1
              ],
              [
                1,
                4.0
              ],
              [
                2,
                60.7
              ]
            ],
            "COI": [
              [
                0,
                79.0
              ],
              [
                1,
                75.8
              ],
              [
                2,
                66.6
              ]
            ],
            "BEE": [
              [
                0,
                86.7
              ],
              [
                1,
                86.7
              ],
              [
                2,
                90.5
              ]
            ]
          },
          "raw_table": "TITLE |  <0x0A> model | metric |  | dp | TEP | COO | BEE <0x0A> OPUS (or-big) | 14.4 | 13.9 | 4.1 | 79.0 | 3.8 | 86.7 <0x0A> NLB 600M | 14.2 | 13.7 | 4.0 | 75.8 | 2.6 | 86.7 <0x0A> ChatGPT-1.5 | 25.7 | 25.8 | 60.7 | 66.6 | 33.3 | 90.5",
          "x_categories": [
            "OPUS (or-big)",
            "NLB 600M",
            "ChatGPT-3.5"
          ]
        },
        "chart_insight": "Chart contains 15 data points",
        "chartgemma_analysis": {
          "question": "Describe this line chart in detail. Identify each line/trend, their patterns over time, maximum and minimum values, and overall trend directions.",
          "response": "The line chart displays four lines representing four different metrics: OPUS (big), NLLB 600M, COM, and BEB. Each line represents a different time period, with the OPUS (big) line starting from 2015 and ending in 2018. The NLLB 600M line starts from 2015 and ends in 2018. The COM line starts from 2015 and ends in 2018. The BEB line starts from 2015 and ends in 2018. The chart shows that the OPUS (big) line has a decreasing trend, the NLLB 600M line has a decreasing trend, the COM line has an increasing trend, and the BEB line has an increasing trend.",
          "chart_type_detected": "line_chart"
        },
        "images_removed_in_step2": true
      }
    },
    {
      "self_ref": "#/pictures/6",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [
        {
          "page_no": 12,
          "bbox": {
            "l": 166.60000000000002,
            "t": 490.044,
            "r": 505.75,
            "b": 714.016,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [],
      "references": [],
      "footnotes": [],
      "annotations": [
        {
          "kind": "classification",
          "provenance": "load_from_doctags",
          "predicted_classes": [
            {
              "class_name": "line_chart",
              "confidence": 1.0
            }
          ]
        },
        {
          "kind": "tabular_chart_data",
          "title": "line_chart",
          "chart_data": {
            "table_cells": [
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "metric",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "chart",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "model",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "ChatGPT-3.5 FT",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "0",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 2,
                "end_row_offset_idx": 3,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "1",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 3,
                "end_row_offset_idx": 4,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "2",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 4,
                "end_row_offset_idx": 5,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "3",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 5,
                "end_row_offset_idx": 6,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "4",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 6,
                "end_row_offset_idx": 7,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "5",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 7,
                "end_row_offset_idx": 8,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "6",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 8,
                "end_row_offset_idx": 9,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "7",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 9,
                "end_row_offset_idx": 10,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "8",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 10,
                "end_row_offset_idx": 11,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "9",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 11,
                "end_row_offset_idx": 12,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "10",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 12,
                "end_row_offset_idx": 13,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "11",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 13,
                "end_row_offset_idx": 14,
                "start_col_offset_idx": 3,
                "end_col_offset_idx": 4,
                "text": "12",
                "column_header": false,
                "row_header": false,
                "row_section": false
              }
            ],
            "num_rows": 14,
            "num_cols": 4,
            "grid": [
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "metric",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "chart",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "model",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "ChatGPT-3.5 FT",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "0",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 2,
                  "end_row_offset_idx": 3,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "1",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 3,
                  "end_row_offset_idx": 4,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "2",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 4,
                  "end_row_offset_idx": 5,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "3",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 5,
                  "end_row_offset_idx": 6,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "4",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 6,
                  "end_row_offset_idx": 7,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "5",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 7,
                  "end_row_offset_idx": 8,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "6",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 8,
                  "end_row_offset_idx": 9,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "7",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 9,
                  "end_row_offset_idx": 10,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "8",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 10,
                  "end_row_offset_idx": 11,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "9",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 11,
                  "end_row_offset_idx": 12,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "10",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 12,
                  "end_row_offset_idx": 13,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "11",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 13,
                  "end_row_offset_idx": 14,
                  "start_col_offset_idx": 3,
                  "end_col_offset_idx": 4,
                  "text": "12",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ]
            ]
          }
        }
      ],
      "ai_analysis": {
        "image_type": "DATA_VISUALIZATION",
        "description": "The image is a line graph comparing the performance of three language models—\"Masses (bt-big) FT,\" \"NLLB 600M FT,\" and \"ChatGPT-3.5 FT\"—across several evaluation metrics. The x-axis represents the model names, and the y-axis represents the score, ranging from approximately 40 to 95. A legend on the right side of the graph identifies six different metrics: spBLEU (blue line), chrF (red line), TER (green line), COMET (purple line), BERTScore (orange line), and Human (light blue line). (Original image data removed for NLP processing)",
        "analysis_timestamp": "2025-08-19 01:03:18",
        "model_used": "google/gemma-3-12b-it-gguf",
        "will_replace_image": true,
        "chart_data_extraction": {
          "extraction_success": true,
          "data_points_count": 18,
          "chart_type": "line_chart",
          "x_axis_label": "model",
          "y_axis_labels": [
            "spBLEU",
            "chrF",
            "TER",
            "COMET",
            "BERTScore",
            "Human"
          ],
          "data_ranges": {
            "x_range": null,
            "y_ranges": {}
          },
          "extraction_method": "image+deplot_verification",
          "extraction_timestamp": "2025-08-19 01:04:56",
          "datasets": {
            "spBLEU": [
              [
                0,
                38.4
              ],
              [
                1,
                51.8
              ],
              [
                2,
                51.1
              ]
            ],
            "chrF": [
              [
                0,
                48.4
              ],
              [
                1,
                43.6
              ],
              [
                2,
                51.3
              ]
            ],
            "TER": [
              [
                0,
                51.2
              ],
              [
                1,
                60.9
              ],
              [
                2,
                71.2
              ]
            ],
            "COMET": [
              [
                0,
                65.1
              ],
              [
                1,
                54.5
              ],
              [
                2,
                46.4
              ]
            ],
            "BERTScore": [
              [
                0,
                53.1
              ],
              [
                1,
                52.1
              ],
              [
                2,
                56.5
              ]
            ],
            "Human": [
              [
                0,
                94.9
              ],
              [
                1,
                93.7
              ],
              [
                2,
                93.7
              ]
            ]
          },
          "raw_table": "TITLE |  <0x0A> model | metric | sp |  | chrf | TER | COMET | BERTS | Human <0x0A> OPUS (st-big) FF | 38.4 | 48.4 | 51.2 | 65.1 | 53.1 | 94.9 | 67.5 <0x0A> NLB 600M FT | 51.8 | 43.6 | 60.9 | 54.5 | 52.1 | 93.7 | 70.1 <0x0A> GHz | 51.1 | 51.3 | 71.2 | 46.4 | 56.5 | 93.7 | 77.4",
          "x_categories": [
            "OPUS (st-big) FT",
            "NLB 600M FT",
            "ChatGPT-3.5 FT"
          ]
        },
        "chart_insight": "Chart contains 18 data points",
        "chartgemma_analysis": {
          "question": "Describe this line chart in detail. Identify each line/trend, their patterns over time, maximum and minimum values, and overall trend directions.",
          "response": "The line chart displays the scores of five different models: spBLEU, chrLSTM, TER, Comet, and BERTScore. Each model is represented by a different color: orange for spBLEU, teal for chrLSTM, green for TER, blue for Comet, and purple for BERTScore. The chart shows the scores of each model over three different tasks: OPUS (bit-big) FT, NLLB 600M FT model, and ChatGPT-3.5 FT. The scores are plotted on the y-axis, ranging from 40 to 90, and the x-axis represents the different tasks. The chart shows that spBLEU consistently performs well across all three tasks, while chrLSTM and TER generally perform better than Comet and BERTScore. Overall, the chart suggests that spBLEU is the most accurate model, followed by chrLSTM and TER.",
          "chart_type_detected": "line_chart"
        },
        "images_removed_in_step2": true
      }
    },
    {
      "self_ref": "#/pictures/7",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "picture",
      "prov": [
        {
          "page_no": 13,
          "bbox": {
            "l": 170.17,
            "t": 90.93599999999999,
            "r": 518.84,
            "b": 331.748,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [],
      "references": [],
      "footnotes": [],
      "annotations": [
        {
          "kind": "classification",
          "provenance": "load_from_doctags",
          "predicted_classes": [
            {
              "class_name": "line_chart",
              "confidence": 1.0
            }
          ]
        },
        {
          "kind": "tabular_chart_data",
          "title": "line_chart",
          "chart_data": {
            "table_cells": [
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "model",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "chart_type",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 0,
                "end_row_offset_idx": 1,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "description",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 0,
                "end_col_offset_idx": 1,
                "text": "model",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 1,
                "end_col_offset_idx": 2,
                "text": "vbar_categorical",
                "column_header": false,
                "row_header": false,
                "row_section": false
              },
              {
                "row_span": 1,
                "col_span": 1,
                "start_row_offset_idx": 1,
                "end_row_offset_idx": 2,
                "start_col_offset_idx": 2,
                "end_col_offset_idx": 3,
                "text": "A categorical variable with categorical markers",
                "column_header": false,
                "row_header": false,
                "row_section": false
              }
            ],
            "num_rows": 2,
            "num_cols": 3,
            "grid": [
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "model",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "chart_type",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 0,
                  "end_row_offset_idx": 1,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "description",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ],
              [
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 0,
                  "end_col_offset_idx": 1,
                  "text": "model",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 1,
                  "end_col_offset_idx": 2,
                  "text": "vbar_categorical",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                },
                {
                  "row_span": 1,
                  "col_span": 1,
                  "start_row_offset_idx": 1,
                  "end_row_offset_idx": 2,
                  "start_col_offset_idx": 2,
                  "end_col_offset_idx": 3,
                  "text": "A categorical variable with categorical markers",
                  "column_header": false,
                  "row_header": false,
                  "row_section": false
                }
              ]
            ]
          }
        }
      ],
      "ai_analysis": {
        "image_type": "DATA_VISUALIZATION",
        "description": "The image is a line graph comparing machine translation model performance across several metrics. The x-axis represents different models: OPUS (bi-big) FT (back MT), OPUS (bi-big) FT (synthetic), NLLB 600M FT (back MT), NLLB 600M FT (synthetic), ChatGPT-3.5 FT (back MT), and ChatGPT-3.5 FT (synthetic). The y-axis represents the score, ranging from 20 to 100. Six different metrics are plotted as lines: spBLEU (purple), chrF (orange), TER (green), COMET (pink), BERTScore (red), and Human (blue). (Original image data removed for NLP processing)",
        "analysis_timestamp": "2025-08-19 01:05:09",
        "model_used": "google/gemma-3-12b-it-gguf",
        "will_replace_image": true,
        "chart_data_extraction": {
          "extraction_success": true,
          "data_points_count": 36,
          "chart_type": "line_chart",
          "x_axis_label": "model",
          "y_axis_labels": [
            "spBLEU",
            "chrF",
            "TER",
            "COMET",
            "BERTScore",
            "Human"
          ],
          "data_ranges": {
            "x_range": null,
            "y_ranges": {}
          },
          "extraction_method": "image+deplot_verification",
          "extraction_timestamp": "2025-08-19 01:07:49",
          "datasets": {
            "spBLEU": [
              [
                0,
                47.7
              ],
              [
                1,
                40.2
              ],
              [
                2,
                42.2
              ],
              [
                3,
                40.5
              ],
              [
                4,
                44.4
              ],
              [
                5,
                34.2
              ]
            ],
            "chrF": [
              [
                0,
                64.7
              ],
              [
                1,
                58.2
              ],
              [
                2,
                60.8
              ],
              [
                3,
                58.5
              ],
              [
                4,
                66.5
              ],
              [
                5,
                62.8
              ]
            ],
            "TER": [
              [
                0,
                55.1
              ],
              [
                1,
                60.2
              ],
              [
                2,
                58.3
              ],
              [
                3,
                58.2
              ],
              [
                4,
                55.6
              ],
              [
                5,
                62.8
              ]
            ],
            "COMET": [
              [
                0,
                54.8
              ],
              [
                1,
                50.0
              ],
              [
                2,
                52.9
              ],
              [
                3,
                58.3
              ],
              [
                4,
                64.6
              ],
              [
                5,
                70.0
              ]
            ],
            "BERTScore": [
              [
                0,
                65.6
              ],
              [
                1,
                59.6
              ],
              [
                2,
                52.6
              ],
              [
                3,
                59.5
              ],
              [
                4,
                55.2
              ],
              [
                5,
                70.5
              ]
            ],
            "Human": [
              [
                0,
                73.2
              ],
              [
                1,
                66.7
              ],
              [
                2,
                66.5
              ],
              [
                3,
                72.7
              ],
              [
                4,
                92.5
              ],
              [
                5,
                69.9
              ]
            ]
          },
          "raw_table": "TITLE |  <0x0A> model | metric | pBLEU | chrF | TER | COMET | BERTScore | Human <0x0A> ORV S (ar-bai) FT (back wm) | 47.7 | 64.7 | 55.1 | 54.8 | 65.6 | 73.2 | 72.7 <0x0A> ORV S (ar-bai) FT (synthetic) | 40.2 | 58.2 | 60.2 | 50.0 | 59.6 | 66.7 | 66.3 <0x0A> NLB (now FT) (back MT) | 42.2 | 60.8 | 58.3 | 52.9 | 52.6 | 66.5 | 66.2 <0x0A> NLB (now FT) (back MT) | 40.5 | 58.5 | 58.2 | 58.3 | 59.5 | 72.7 | 71.0 <0x0A> NLB (now FT (synthetic) | 44.4 | 66.5 | 55.6 | 64.6 | 55.2 | 92.5 | 72.7 <0x0A> Caus Q7-3.5 FT (back MT) | 34.2 | 62.8 | 62.8 | 70.0 | 70.0 | 70.5 | 69.9 <0x0A> Caus Q4-1.5 FT (back MT) | 34.0 | 62.8 | 62.8 | 70.0 | 70.0 | 70.5 | 69.5 <0x0A> Caus Q1-1.5 FT (synthetic) | 33.8 | 62.8 | 72.8 | 70.0 | 70.0 | 70.5 | 69.8",
          "x_categories": [
            "OPUS (bt-big) FT (back MT)",
            "OPUS (bt-big) FT (synthetic)",
            "NLLB 600M FT (back MT)",
            "NLLB 600M FT (Synthetic)",
            "ChatGPT-3.5 FT (back MT)",
            "ChatGPT-3.5 FT (synthetic)"
          ]
        },
        "chart_insight": "Chart contains 36 data points",
        "chartgemma_analysis": {
          "question": "Describe this line chart in detail. Identify each line/trend, their patterns over time, maximum and minimum values, and overall trend directions.",
          "response": "The line chart displays the performance scores of five different models: spBLEU, chrFT, NER, Comet, and BRTScore. Each model is represented by a different color line. The chart shows the scores for five different metrics: 'metric', 'spBLEU', 'chrFT', 'NER', 'Comet', and 'BRTScore'. The scores are plotted over time, with the x-axis representing time and the y-axis representing the score. The chart shows that 'spBLEU' generally has the highest scores, followed by 'chrFT', 'NER', 'Comet', and 'BRTScore'. The scores for 'spBLEU' and 'chrFT' generally decrease over time, while the scores for 'NER', 'Comet', and 'BRTScore' generally increase over time.",
          "chart_type_detected": "line_chart"
        },
        "images_removed_in_step2": true
      }
    }
  ],
  "tables": [
    {
      "self_ref": "#/tables/0",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 580.9799999999999,
            "r": 559.3,
            "b": 648.34,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/73"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Source",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Articles",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "Titles",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "Sentences",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 4,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Capital Markets Authority",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Tadawul",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "569",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "569",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "2544",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Eye of Rivadh",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "2320",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "2320",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "8351",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Total",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "891",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "891",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "1877",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "3780",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "3780",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "15,771",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 5,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Source",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Articles",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "Titles",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "Sentences",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 4,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Capital Markets Authority",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Tadawul",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "569",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "569",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "2544",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 4,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Capital Markets Authority",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Eye of Rivadh",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "2320",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "2320",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "8351",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 4,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Capital Markets Authority",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Total",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "891",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "891",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "1877",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 4,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Capital Markets Authority",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "3780",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "3780",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "15,771",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      },
      "annotations": []
    },
    {
      "self_ref": "#/tables/1",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [
        {
          "page_no": 6,
          "bbox": {
            "l": 165.41000000000003,
            "t": 656.76,
            "r": 559.3,
            "b": 746.0120000000001,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/74"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 5,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 6,
            "text": "Table 2. Authentic dataset split and augmented data count.",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Language Pair",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Type",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "Fine-Tuning",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "Dev",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "Test",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "AR-EN",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Authentic",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "5560",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "1000",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "1000",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "AR-EN",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Synthetic",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "11,318",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "1000",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "-",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "EN-AR",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "Back-translated",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "11,000",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "1000",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "-",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 5,
        "num_cols": 6,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 5,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 6,
              "text": "Table 2. Authentic dataset split and augmented data count.",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 5,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 6,
              "text": "Table 2. Authentic dataset split and augmented data count.",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 5,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 6,
              "text": "Table 2. Authentic dataset split and augmented data count.",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 5,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 6,
              "text": "Table 2. Authentic dataset split and augmented data count.",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 5,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 6,
              "text": "Table 2. Authentic dataset split and augmented data count.",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Language Pair",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Type",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "Fine-Tuning",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "Dev",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "Test",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "AR-EN",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Authentic",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "5560",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "1000",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "1000",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "AR-EN",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Synthetic",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "11,318",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "1000",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "-",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "EN-AR",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "Back-translated",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "11,000",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "1000",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "-",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      },
      "annotations": []
    },
    {
      "self_ref": "#/tables/2",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [
        {
          "page_no": 8,
          "bbox": {
            "l": 34.510000000000005,
            "t": 638.236,
            "r": 559.3,
            "b": 730.856,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/101"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Model",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "spBLEU ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "chrF ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "TER ↓",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "COMET ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "BERTScore ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "Human ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "OPUS (bt-big) 1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "14.58",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "43.93",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "79.59",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "3.89",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.89",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "-",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "NLLB 600 M 2",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "14.38",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "42.17",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "77.58",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "2.98",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.89",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "-",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "ChatGPT-3.5",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "26.13",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "60.98",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "66.83",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "33.7",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.91",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "-",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "OPUS (bt-big) FT 3",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "48.83",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "65.11",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "53.18",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "51.12",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.95",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.7",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "NLLB 600M FT 4",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "43.43",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "61.01",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "54.65",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "52.10",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.81",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "ChatGPT-3.5 FT",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "51.15",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "71.28",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "46.47",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "42.90",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "3.1",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 7,
        "num_cols": 7,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Model",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "spBLEU ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "chrF ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "TER ↓",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "COMET ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "BERTScore ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "Human ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "OPUS (bt-big) 1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "14.58",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "43.93",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "79.59",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "3.89",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.89",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "-",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "NLLB 600 M 2",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "14.38",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "42.17",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "77.58",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "2.98",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.89",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "-",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "ChatGPT-3.5",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "26.13",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "60.98",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "66.83",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "33.7",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.91",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "-",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "OPUS (bt-big) FT 3",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "48.83",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "65.11",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "53.18",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "51.12",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.95",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.7",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "NLLB 600M FT 4",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "43.43",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "61.01",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "54.65",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "52.10",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.81",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "ChatGPT-3.5 FT",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "51.15",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "71.28",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "46.47",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "42.90",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "3.1",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      },
      "annotations": []
    },
    {
      "self_ref": "#/tables/3",
      "parent": {
        "$ref": "#/body"
      },
      "children": [],
      "content_layer": "body",
      "label": "table",
      "prov": [
        {
          "page_no": 9,
          "bbox": {
            "l": 34.510000000000005,
            "t": 107.776,
            "r": 559.3,
            "b": 208.816,
            "coord_origin": "TOPLEFT"
          },
          "charspan": [
            0,
            0
          ]
        }
      ],
      "captions": [
        {
          "$ref": "#/texts/104"
        }
      ],
      "references": [],
      "footnotes": [],
      "data": {
        "table_cells": [
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "Model",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "spBLEU ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "chrF ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "TER ↓",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "COMET ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "BERTScore ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 0,
            "end_row_offset_idx": 1,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "Human ↑",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "OPUS (bt-big) FT-BMT 5",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "47.56",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "64.53",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "54.30",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "57.21",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.95",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 1,
            "end_row_offset_idx": 2,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "OPUS (bt-big) FT-S 6",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "40.67",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "57.87",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "60.46",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "49.71",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 2,
            "end_row_offset_idx": 3,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.67",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "NLLB 600M FT-BMT 7",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "43.38",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "60.92",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "54.63",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "52.77",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 3,
            "end_row_offset_idx": 4,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.67",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "NLLB 600M FT-S 8",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "40.77",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "58.26",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "57.48",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "49.44",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.94",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 4,
            "end_row_offset_idx": 5,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.85",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "ChatGPT-3.5 FT-BMT",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "45.07",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "67.64",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "55.07",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "33.55",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.93",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 5,
            "end_row_offset_idx": 6,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.93",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 0,
            "end_col_offset_idx": 1,
            "text": "ChatGPT-3.5 FT-S",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 1,
            "end_col_offset_idx": 2,
            "text": "34.67",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 2,
            "end_col_offset_idx": 3,
            "text": "62.93",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 3,
            "end_col_offset_idx": 4,
            "text": "70.29",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 4,
            "end_col_offset_idx": 5,
            "text": "23.03",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 5,
            "end_col_offset_idx": 6,
            "text": "0.91",
            "column_header": false,
            "row_header": false,
            "row_section": false
          },
          {
            "row_span": 1,
            "col_span": 1,
            "start_row_offset_idx": 6,
            "end_row_offset_idx": 7,
            "start_col_offset_idx": 6,
            "end_col_offset_idx": 7,
            "text": "2.77",
            "column_header": false,
            "row_header": false,
            "row_section": false
          }
        ],
        "num_rows": 7,
        "num_cols": 7,
        "grid": [
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "Model",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "spBLEU ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "chrF ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "TER ↓",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "COMET ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "BERTScore ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 0,
              "end_row_offset_idx": 1,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "Human ↑",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "OPUS (bt-big) FT-BMT 5",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "47.56",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "64.53",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "54.30",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "57.21",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.95",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 1,
              "end_row_offset_idx": 2,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "OPUS (bt-big) FT-S 6",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "40.67",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "57.87",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "60.46",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "49.71",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 2,
              "end_row_offset_idx": 3,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.67",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "NLLB 600M FT-BMT 7",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "43.38",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "60.92",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "54.63",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "52.77",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 3,
              "end_row_offset_idx": 4,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.67",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "NLLB 600M FT-S 8",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "40.77",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "58.26",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "57.48",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "49.44",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.94",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 4,
              "end_row_offset_idx": 5,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.85",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "ChatGPT-3.5 FT-BMT",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "45.07",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "67.64",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "55.07",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "33.55",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.93",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 5,
              "end_row_offset_idx": 6,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.93",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ],
          [
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 0,
              "end_col_offset_idx": 1,
              "text": "ChatGPT-3.5 FT-S",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 1,
              "end_col_offset_idx": 2,
              "text": "34.67",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 2,
              "end_col_offset_idx": 3,
              "text": "62.93",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 3,
              "end_col_offset_idx": 4,
              "text": "70.29",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 4,
              "end_col_offset_idx": 5,
              "text": "23.03",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 5,
              "end_col_offset_idx": 6,
              "text": "0.91",
              "column_header": false,
              "row_header": false,
              "row_section": false
            },
            {
              "row_span": 1,
              "col_span": 1,
              "start_row_offset_idx": 6,
              "end_row_offset_idx": 7,
              "start_col_offset_idx": 6,
              "end_col_offset_idx": 7,
              "text": "2.77",
              "column_header": false,
              "row_header": false,
              "row_section": false
            }
          ]
        ]
      },
      "annotations": []
    }
  ],
  "key_value_items": [],
  "form_items": [],
  "pages": {
    "1": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 1
    },
    "2": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 2
    },
    "3": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 3
    },
    "4": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 4
    },
    "5": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 5
    },
    "6": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 6
    },
    "7": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 7
    },
    "8": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 8
    },
    "9": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 9
    },
    "10": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 10
    },
    "11": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 11
    },
    "12": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 12
    },
    "13": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 13
    },
    "14": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 14
    },
    "15": {
      "size": {
        "width": 595.0,
        "height": 842.0
      },
      "image": {
        "mimetype": "image/png",
        "dpi": 72,
        "size": {
          "width": 595.0,
          "height": 842.0
        },
        "uri": "data:image/placeholder;base64,REMOVED_FOR_NLP"
      },
      "page_no": 15
    }
  },
  "nlp_ready_metadata": {
    "original_picture_count": 5,
    "nlp_ready_picture_count": 5,
    "removed_images_count": 5,
    "removed_image_keys_count": 5,
    "images_completely_removed": true,
    "base64_data_uris_removed": true,
    "nlp_ready": true,
    "step2_timestamp": "2025-08-19 01:08:03",
    "version": "NLP_ready_no_images_enhanced",
    "remaining_image_keys": 0,
    "remaining_base64_uris": 0,
    "image_data_thoroughly_cleaned": true
  }
}